# HealthBench Evaluation

This directory contains the experiment for evaluating large language models using the HealthBench benchmark. The evaluation is focused on distinguishing between factual and hallucinated content generated by the models.

## Experimental Workflow

### 1. Run Model Ensemble (`1-Run_ensemble/`)

The first step is to run an ensemble of models on the HealthBench dataset. The script `forced_ensemble_models.py` is used for this purpose. The experiment is run under two conditions:
- `Paragraph_fact/`: For generating factual paragraphs.
- `Paragraph_hallucination/`: For generating paragraphs containing hallucinations.

The data required for this step is located in the `Data/` directory and needs to be downloaded from Zenodo.

### 2. Process Ensemble Outputs (`2-Proccess_ensemble/`)

The outputs from the model ensemble are processed to extract features and prepare the data for analysis. This involves:
- `1-features_creation.ipynb`: Notebook for creating features from the generated text. The results are stored in the `Features/` directory.
- `2-KL_div.ipynb`: Notebook for calculating KL divergence.
- `3-Reduce_and_concatenate_Paragraph-fact.ipynb` and `3-Reduce_and_concatenate_Paragraph-hallucination.ipynb`: Notebooks to process and combine the results for both factual and hallucination arms of the experiment.

### 3. Analysis (`3-Analysis/`)

The final step is to analyze the processed data to evaluate model performance on the HealthBench benchmark. The notebooks for this analysis are located in this directory.

### Supporting Files and Directories

- `Data/`: Contains the HealthBench data and scripts to generate hallucinated (`generate_Hallucination.py`) and rephrased (`generate_rephrase.py`) examples.
- `Features/`: Stores the features extracted from the model outputs.
