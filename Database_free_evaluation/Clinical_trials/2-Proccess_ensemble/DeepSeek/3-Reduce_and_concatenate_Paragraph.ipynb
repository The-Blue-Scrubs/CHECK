{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a0207-4cd6-4782-b568-5d972d2f2cfb",
   "metadata": {},
   "source": [
    "## Here we reduce and concatenate the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5445748e-6fea-4172-82a7-c55c5867bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1499 CSV files to process\n",
      "Output will have 681 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [02:41<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1499 CSV files to concatenate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [00:10<00:00, 140.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame info:\n",
      "Shape: (1499, 683)\n",
      "Number of samples: 1499\n",
      "Number of features: 683\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "hallucination    1150\n",
      "fact              347\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_fixed_column_names():\n",
    "    \"\"\"\n",
    "    Generate a fixed list of column names to ensure consistency across all files\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    \n",
    "    # Models (excluding model_5)\n",
    "    models = [f\"model_{i}\" for i in range(1, 7) if i != 5]\n",
    "    \n",
    "    # Metrics for each model\n",
    "    metrics = ['gold_lp', 'rank', 'Entropy', 'gold_lp_cumsum', 'gold_prob', 'gold_lp_normalized']\n",
    "    \n",
    "    # Statistics to compute\n",
    "    stats = ['median', 'max', 'min', 'std', \n",
    "             'moment1', 'moment2', 'moment3', 'moment4', 'moment5', \n",
    "             'q95', 'q90', 'q85', 'q80', 'q20', 'q15', 'q10', 'q05']\n",
    "    \n",
    "    # Generate columns for model metrics\n",
    "    for model in models:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{model}_{metric}_{stat}\")\n",
    "    \n",
    "    # Generate columns for KL divergence terms\n",
    "    model_pairs = [(i, j) for i in range(1, 7) for j in range(i+1, 7) \n",
    "                  if i != 5 and j != 5]\n",
    "    \n",
    "    for i, j in model_pairs:\n",
    "        for stat in stats:\n",
    "            columns.append(f\"kl_{i}_vs_{j}_{stat}\")\n",
    "    \n",
    "    return columns\n",
    "\n",
    "def get_label_from_json(trial_name, question_number, label_folder_path):\n",
    "    \"\"\"\n",
    "    Get label from corresponding JSON file if label_folder_path is provided\n",
    "    \"\"\"\n",
    "    if not label_folder_path:  # If no path provided\n",
    "        return None\n",
    "        \n",
    "    json_path = os.path.join(\n",
    "        label_folder_path,\n",
    "        f\"{trial_name}.json\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            label = data[f\"Q{question_number}\"][\"eval_databases\"]\n",
    "            # Explicitly handle \"N/A\" to prevent pandas from converting it\n",
    "            if label == \"N/A\":\n",
    "                return \"N/A\"  # Forces pandas to keep it as string\n",
    "            return label\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading JSON for {trial_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_trial_and_question(file_path):\n",
    "    \"\"\"\n",
    "    Extract trial name and question number from file path\n",
    "    Example: path/to/NCT00001959/logprob_matrix_1.csv -> (\"NCT00001959\", 1)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get filename and remove extension\n",
    "        filename = os.path.basename(file_path)\n",
    "        # Extract question number\n",
    "        question_num = int(filename.split('_')[-1].split('.')[0])\n",
    "        # Extract trial name from path\n",
    "        trial_name = file_path.split('/')[-2]  # Adjust this based on your actual path structure\n",
    "        return trial_name, question_num\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting trial and question from {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def compute_statistics(series):\n",
    "    \"\"\"\n",
    "    Compute various statistical measures for a series\n",
    "    \"\"\"\n",
    "    mean = series.mean()\n",
    "    \n",
    "    stats = {\n",
    "        'median': series.median(),\n",
    "        'max': series.max(),\n",
    "        'min': series.min(),\n",
    "        'std': series.std(),\n",
    "        \n",
    "        # Central moments\n",
    "        'moment1': mean,\n",
    "        'moment2': ((series - mean) ** 2).mean(),\n",
    "        'moment3': ((series - mean) ** 3).mean(),\n",
    "        'moment4': ((series - mean) ** 4).mean(),\n",
    "        'moment5': ((series - mean) ** 5).mean(),\n",
    "        \n",
    "        # Existing quantiles\n",
    "        'q95': series.quantile(0.95),\n",
    "        'q90': series.quantile(0.90),\n",
    "        'q85': series.quantile(0.85),\n",
    "        'q80': series.quantile(0.80),\n",
    "        'q20': series.quantile(0.20),\n",
    "        'q15': series.quantile(0.15),\n",
    "        'q10': series.quantile(0.10),\n",
    "        'q05': series.quantile(0.05),\n",
    "    }    \n",
    "    return stats\n",
    "\n",
    "def process_metrics_and_kl(input_folder, kl_folder, output_folder, label_folder_path=None):\n",
    "    \"\"\"\n",
    "    Process original metrics and KL divergence terms with consistent column ordering\n",
    "    \"\"\"\n",
    "    # Get fixed column names\n",
    "    fixed_columns = get_fixed_column_names()\n",
    "    \n",
    "    # Original metrics to analyze\n",
    "    metrics = ['gold_lp', 'rank', 'Entropy', 'gold_lp_cumsum', 'gold_prob', 'gold_lp_normalized']\n",
    "    \n",
    "    # Get all CSV files from model1 directory\n",
    "    csv_files = []\n",
    "    model1_path = os.path.join(input_folder, \"model_1\")\n",
    "    for root, dirs, files in os.walk(model1_path):\n",
    "        # Skip hidden directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for file in files:\n",
    "            if not file.startswith('.') and file.endswith('.csv'):\n",
    "                csv_files.append((os.path.join(root, file), file))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    print(f\"Output will have {len(fixed_columns) + 1} columns\")  # +1 for label\n",
    "    \n",
    "    for file_path1, file_name in tqdm(csv_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            # Get trial name and question number\n",
    "            trial_name, question_num = extract_trial_and_question(file_path1)\n",
    "            \n",
    "            if trial_name and question_num and label_folder_path:\n",
    "                # Get label from JSON only if path is provided\n",
    "                label = get_label_from_json(trial_name, question_num, label_folder_path)\n",
    "            else:\n",
    "                label = None\n",
    "            \n",
    "            # Dictionary to store all metrics\n",
    "            all_stats = {col: np.nan for col in fixed_columns}  # Initialize with NaN\n",
    "            \n",
    "            # Add label column\n",
    "            all_stats['label'] = label\n",
    "            \n",
    "            # Process each model (excluding model_5)\n",
    "            models = [f\"model_{i}\" for i in range(1, 7) if i != 5]\n",
    "            \n",
    "            # Step 1: Process original metrics for each model\n",
    "            for model in models:\n",
    "                file_path = os.path.join(input_folder, model, os.path.relpath(file_path1, model1_path))\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"Skipping {file_name} - no matching file in {model}\")\n",
    "                    continue\n",
    "                \n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Compute statistics for each metric\n",
    "                for metric in metrics:\n",
    "                    if metric in df.columns:\n",
    "                        stats = compute_statistics(df[metric])\n",
    "                        for stat_name, value in stats.items():\n",
    "                            col_name = f\"{model}_{metric}_{stat_name}\"\n",
    "                            all_stats[col_name] = value\n",
    "            \n",
    "            # Step 2: Process KL divergence terms\n",
    "            kl_file_path = os.path.join(kl_folder, os.path.relpath(file_path1, model1_path))\n",
    "            if os.path.exists(kl_file_path):\n",
    "                kl_df = pd.read_csv(kl_file_path)\n",
    "                \n",
    "                # Get all KL columns\n",
    "                kl_cols = [col for col in kl_df.columns if col.startswith('kl_')]\n",
    "                \n",
    "                # Compute statistics for each KL term\n",
    "                for kl_col in kl_cols:\n",
    "                    stats = compute_statistics(kl_df[kl_col])\n",
    "                    for stat_name, value in stats.items():\n",
    "                        col_name = f\"{kl_col}_{stat_name}\"\n",
    "                        all_stats[col_name] = value\n",
    "            \n",
    "            # Create output DataFrame with fixed column order plus label\n",
    "            columns_with_label = fixed_columns + ['label']\n",
    "            result_df = pd.DataFrame([all_stats])[columns_with_label]\n",
    "            \n",
    "            # Create output directory structure\n",
    "            output_file_path = os.path.join(output_folder, os.path.relpath(file_path1, model1_path))\n",
    "            os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "            \n",
    "            # Save results\n",
    "            result_df.to_csv(output_file_path, index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {file_name}:\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "def concatenate_output_files(output_folder):\n",
    "    \"\"\"\n",
    "    Concatenate all CSV files in Output_folder and its subfolders into a single DataFrame\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(output_folder):\n",
    "        # Skip hidden directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for file in files:\n",
    "            if not file.startswith('.') and file.endswith('.csv'):\n",
    "                csv_files.append((os.path.join(root, file), file))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to concatenate\")\n",
    "    # Sort files alphabetically\n",
    "    csv_files = sorted(csv_files)\n",
    "    \n",
    "    # Read and concatenate all files\n",
    "    all_dfs = []\n",
    "    for file_path, file_name in tqdm(csv_files, desc=\"Reading files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Add filename as a column (optional but useful for tracking)\n",
    "            df['source_file'] = file_name\n",
    "            \n",
    "            # Add full path as a column (optional)\n",
    "            df['file_path'] = os.path.relpath(file_path, output_folder)\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError reading {file_name}:\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        # Print some information about the final DataFrame\n",
    "        print(\"\\nFinal DataFrame info:\")\n",
    "        print(f\"Shape: {final_df.shape}\")\n",
    "        print(f\"Number of samples: {len(final_df)}\")\n",
    "        print(f\"Number of features: {len(final_df.columns)}\")\n",
    "        \n",
    "        if 'label' in final_df.columns:\n",
    "            print(\"\\nLabel distribution:\")\n",
    "            print(final_df['label'].value_counts())\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No files were successfully read!\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    input_folder = \"Database_free_evaluation/Clinical_trials/Features/FEATURES_DeepSeek\"\n",
    "    kl_folder = \"kl_analysis_Paragraph_title_features\"\n",
    "    output_folder = \"Output_folder_Paragraph_title\"\n",
    "\n",
    "     # Label folder path (set to None if not using labels)\n",
    "    label_folder_path = \"Database_dependent_evaluation/Clinical_trials/4-Evaluation/Evaluation_factual/DeepSeek/Paragraph_level/Evaluation_title_Paragraph_GPT\"\n",
    "    # Or for other folders:\n",
    "    # label_folder_path = None\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Step 1: Process files and create individual outputs\n",
    "    process_metrics_and_kl(input_folder, kl_folder, output_folder, label_folder_path)\n",
    "\n",
    "    # Step 2: Concatenate all output files\n",
    "    final_df = concatenate_output_files(output_folder)\n",
    "    \n",
    "    # Optional: Save concatenated DataFrame\n",
    "    if final_df is not None:\n",
    "        final_df.to_csv(\"concatenated_results_Paragraph_title.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8f55d-5f71-4255-b188-02cbed45b642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccab0e6f-224e-483e-b3d5-967756182838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_1_gold_lp_median</th>\n",
       "      <th>model_1_gold_lp_max</th>\n",
       "      <th>model_1_gold_lp_min</th>\n",
       "      <th>model_1_gold_lp_std</th>\n",
       "      <th>model_1_gold_lp_moment1</th>\n",
       "      <th>model_1_gold_lp_moment2</th>\n",
       "      <th>model_1_gold_lp_moment3</th>\n",
       "      <th>model_1_gold_lp_moment4</th>\n",
       "      <th>model_1_gold_lp_moment5</th>\n",
       "      <th>model_1_gold_lp_q95</th>\n",
       "      <th>...</th>\n",
       "      <th>kl_4_vs_6_q90</th>\n",
       "      <th>kl_4_vs_6_q85</th>\n",
       "      <th>kl_4_vs_6_q80</th>\n",
       "      <th>kl_4_vs_6_q20</th>\n",
       "      <th>kl_4_vs_6_q15</th>\n",
       "      <th>kl_4_vs_6_q10</th>\n",
       "      <th>kl_4_vs_6_q05</th>\n",
       "      <th>label</th>\n",
       "      <th>source_file</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.116863</td>\n",
       "      <td>1.650339</td>\n",
       "      <td>-0.376421</td>\n",
       "      <td>2.695541</td>\n",
       "      <td>-29.291987</td>\n",
       "      <td>3.788929e+02</td>\n",
       "      <td>-5.101671e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305703</td>\n",
       "      <td>0.202104</td>\n",
       "      <td>0.088715</td>\n",
       "      <td>3.320311e-07</td>\n",
       "      <td>1.460378e-07</td>\n",
       "      <td>4.908980e-08</td>\n",
       "      <td>8.288604e-09</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_1.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>10.887073</td>\n",
       "      <td>-2.401454</td>\n",
       "      <td>117.225849</td>\n",
       "      <td>-10379.188407</td>\n",
       "      <td>1.001059e+06</td>\n",
       "      <td>-9.740825e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378235</td>\n",
       "      <td>0.266549</td>\n",
       "      <td>0.217734</td>\n",
       "      <td>3.457059e-06</td>\n",
       "      <td>2.413533e-06</td>\n",
       "      <td>4.376253e-07</td>\n",
       "      <td>7.058479e-08</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_10.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_10.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.030774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>10.305379</td>\n",
       "      <td>-2.077365</td>\n",
       "      <td>105.105992</td>\n",
       "      <td>-9703.605439</td>\n",
       "      <td>9.482396e+05</td>\n",
       "      <td>-9.282447e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379517</td>\n",
       "      <td>0.281111</td>\n",
       "      <td>0.199646</td>\n",
       "      <td>6.378914e-06</td>\n",
       "      <td>2.484864e-06</td>\n",
       "      <td>4.853980e-07</td>\n",
       "      <td>2.474919e-08</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_11.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_11.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.559920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>20.410269</td>\n",
       "      <td>-6.222168</td>\n",
       "      <td>412.924882</td>\n",
       "      <td>-36061.273966</td>\n",
       "      <td>3.393322e+06</td>\n",
       "      <td>-3.181027e+08</td>\n",
       "      <td>-3.516673e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477269</td>\n",
       "      <td>0.383312</td>\n",
       "      <td>0.314148</td>\n",
       "      <td>6.137047e-03</td>\n",
       "      <td>9.056092e-04</td>\n",
       "      <td>1.569917e-04</td>\n",
       "      <td>1.054056e-05</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_12.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_12.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.005173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>18.055005</td>\n",
       "      <td>-4.175513</td>\n",
       "      <td>320.550147</td>\n",
       "      <td>-29277.356694</td>\n",
       "      <td>2.810752e+06</td>\n",
       "      <td>-2.693146e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387154</td>\n",
       "      <td>0.266365</td>\n",
       "      <td>0.183391</td>\n",
       "      <td>2.153387e-06</td>\n",
       "      <td>1.439171e-06</td>\n",
       "      <td>1.348659e-07</td>\n",
       "      <td>2.859716e-08</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_13.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_13.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.882793</td>\n",
       "      <td>1.950844</td>\n",
       "      <td>-0.699867</td>\n",
       "      <td>3.777389</td>\n",
       "      <td>-27.615960</td>\n",
       "      <td>2.662460e+02</td>\n",
       "      <td>-2.770496e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352078</td>\n",
       "      <td>0.236819</td>\n",
       "      <td>0.155296</td>\n",
       "      <td>7.688756e-07</td>\n",
       "      <td>3.125340e-07</td>\n",
       "      <td>5.273651e-08</td>\n",
       "      <td>3.380948e-08</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_5.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_5.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-0.018668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>8.843934</td>\n",
       "      <td>-1.545903</td>\n",
       "      <td>77.618114</td>\n",
       "      <td>-7295.455760</td>\n",
       "      <td>7.173414e+05</td>\n",
       "      <td>-7.061592e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299087</td>\n",
       "      <td>0.228928</td>\n",
       "      <td>0.146895</td>\n",
       "      <td>3.523717e-06</td>\n",
       "      <td>1.225588e-06</td>\n",
       "      <td>4.676687e-07</td>\n",
       "      <td>2.930865e-08</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_6.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_6.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>-0.010285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.905843</td>\n",
       "      <td>3.115767</td>\n",
       "      <td>-1.104159</td>\n",
       "      <td>8.899002</td>\n",
       "      <td>-77.539064</td>\n",
       "      <td>7.701772e+02</td>\n",
       "      <td>-7.538058e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702936</td>\n",
       "      <td>0.499897</td>\n",
       "      <td>0.357136</td>\n",
       "      <td>4.996134e-07</td>\n",
       "      <td>2.936043e-07</td>\n",
       "      <td>1.522245e-07</td>\n",
       "      <td>7.603017e-08</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_7.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_7.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>-0.078338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>12.523537</td>\n",
       "      <td>-2.878957</td>\n",
       "      <td>156.017839</td>\n",
       "      <td>-14388.508151</td>\n",
       "      <td>1.397616e+06</td>\n",
       "      <td>-1.357242e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473413</td>\n",
       "      <td>0.392672</td>\n",
       "      <td>0.246967</td>\n",
       "      <td>1.552832e-05</td>\n",
       "      <td>4.076553e-06</td>\n",
       "      <td>1.474322e-06</td>\n",
       "      <td>1.616534e-07</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_8.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_8.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>-0.114498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>9.161398</td>\n",
       "      <td>-2.082975</td>\n",
       "      <td>83.295370</td>\n",
       "      <td>-7235.170611</td>\n",
       "      <td>6.988719e+05</td>\n",
       "      <td>-6.824070e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406111</td>\n",
       "      <td>0.320968</td>\n",
       "      <td>0.282226</td>\n",
       "      <td>1.002641e-04</td>\n",
       "      <td>2.020167e-05</td>\n",
       "      <td>4.476000e-06</td>\n",
       "      <td>2.467256e-07</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_9.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_9.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1499 rows × 683 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_1_gold_lp_median  model_1_gold_lp_max  model_1_gold_lp_min  \\\n",
       "0                  -0.000016                  0.0           -14.116863   \n",
       "1                  -0.007329                  0.0          -100.000000   \n",
       "2                  -0.030774                  0.0          -100.000000   \n",
       "3                  -0.559920                  0.0          -100.000000   \n",
       "4                  -0.005173                  0.0          -100.000000   \n",
       "...                      ...                  ...                  ...   \n",
       "1494               -0.000003                  0.0           -12.882793   \n",
       "1495               -0.018668                  0.0          -100.000000   \n",
       "1496               -0.010285                  0.0           -10.905843   \n",
       "1497               -0.078338                  0.0          -100.000000   \n",
       "1498               -0.114498                  0.0          -100.000000   \n",
       "\n",
       "      model_1_gold_lp_std  model_1_gold_lp_moment1  model_1_gold_lp_moment2  \\\n",
       "0                1.650339                -0.376421                 2.695541   \n",
       "1               10.887073                -2.401454               117.225849   \n",
       "2               10.305379                -2.077365               105.105992   \n",
       "3               20.410269                -6.222168               412.924882   \n",
       "4               18.055005                -4.175513               320.550147   \n",
       "...                   ...                      ...                      ...   \n",
       "1494             1.950844                -0.699867                 3.777389   \n",
       "1495             8.843934                -1.545903                77.618114   \n",
       "1496             3.115767                -1.104159                 8.899002   \n",
       "1497            12.523537                -2.878957               156.017839   \n",
       "1498             9.161398                -2.082975                83.295370   \n",
       "\n",
       "      model_1_gold_lp_moment3  model_1_gold_lp_moment4  \\\n",
       "0                  -29.291987             3.788929e+02   \n",
       "1               -10379.188407             1.001059e+06   \n",
       "2                -9703.605439             9.482396e+05   \n",
       "3               -36061.273966             3.393322e+06   \n",
       "4               -29277.356694             2.810752e+06   \n",
       "...                       ...                      ...   \n",
       "1494               -27.615960             2.662460e+02   \n",
       "1495             -7295.455760             7.173414e+05   \n",
       "1496               -77.539064             7.701772e+02   \n",
       "1497            -14388.508151             1.397616e+06   \n",
       "1498             -7235.170611             6.988719e+05   \n",
       "\n",
       "      model_1_gold_lp_moment5  model_1_gold_lp_q95  ...  kl_4_vs_6_q90  \\\n",
       "0               -5.101671e+03         0.000000e+00  ...       0.305703   \n",
       "1               -9.740825e+07         0.000000e+00  ...       0.378235   \n",
       "2               -9.282447e+07         0.000000e+00  ...       0.379517   \n",
       "3               -3.181027e+08        -3.516673e-07  ...       0.477269   \n",
       "4               -2.693146e+08         0.000000e+00  ...       0.387154   \n",
       "...                       ...                  ...  ...            ...   \n",
       "1494            -2.770496e+03         0.000000e+00  ...       0.352078   \n",
       "1495            -7.061592e+07         0.000000e+00  ...       0.299087   \n",
       "1496            -7.538058e+03         0.000000e+00  ...       0.702936   \n",
       "1497            -1.357242e+08         0.000000e+00  ...       0.473413   \n",
       "1498            -6.824070e+07         0.000000e+00  ...       0.406111   \n",
       "\n",
       "      kl_4_vs_6_q85  kl_4_vs_6_q80  kl_4_vs_6_q20  kl_4_vs_6_q15  \\\n",
       "0          0.202104       0.088715   3.320311e-07   1.460378e-07   \n",
       "1          0.266549       0.217734   3.457059e-06   2.413533e-06   \n",
       "2          0.281111       0.199646   6.378914e-06   2.484864e-06   \n",
       "3          0.383312       0.314148   6.137047e-03   9.056092e-04   \n",
       "4          0.266365       0.183391   2.153387e-06   1.439171e-06   \n",
       "...             ...            ...            ...            ...   \n",
       "1494       0.236819       0.155296   7.688756e-07   3.125340e-07   \n",
       "1495       0.228928       0.146895   3.523717e-06   1.225588e-06   \n",
       "1496       0.499897       0.357136   4.996134e-07   2.936043e-07   \n",
       "1497       0.392672       0.246967   1.552832e-05   4.076553e-06   \n",
       "1498       0.320968       0.282226   1.002641e-04   2.020167e-05   \n",
       "\n",
       "      kl_4_vs_6_q10  kl_4_vs_6_q05          label            source_file  \\\n",
       "0      4.908980e-08   8.288604e-09  hallucination   logprob_matrix_1.csv   \n",
       "1      4.376253e-07   7.058479e-08  hallucination  logprob_matrix_10.csv   \n",
       "2      4.853980e-07   2.474919e-08  hallucination  logprob_matrix_11.csv   \n",
       "3      1.569917e-04   1.054056e-05  hallucination  logprob_matrix_12.csv   \n",
       "4      1.348659e-07   2.859716e-08  hallucination  logprob_matrix_13.csv   \n",
       "...             ...            ...            ...                    ...   \n",
       "1494   5.273651e-08   3.380948e-08  hallucination   logprob_matrix_5.csv   \n",
       "1495   4.676687e-07   2.930865e-08  hallucination   logprob_matrix_6.csv   \n",
       "1496   1.522245e-07   7.603017e-08           fact   logprob_matrix_7.csv   \n",
       "1497   1.474322e-06   1.616534e-07  hallucination   logprob_matrix_8.csv   \n",
       "1498   4.476000e-06   2.467256e-07  hallucination   logprob_matrix_9.csv   \n",
       "\n",
       "                              file_path  \n",
       "0      NCT00001959/logprob_matrix_1.csv  \n",
       "1     NCT00001959/logprob_matrix_10.csv  \n",
       "2     NCT00001959/logprob_matrix_11.csv  \n",
       "3     NCT00001959/logprob_matrix_12.csv  \n",
       "4     NCT00001959/logprob_matrix_13.csv  \n",
       "...                                 ...  \n",
       "1494   NCT04739800/logprob_matrix_5.csv  \n",
       "1495   NCT04739800/logprob_matrix_6.csv  \n",
       "1496   NCT04739800/logprob_matrix_7.csv  \n",
       "1497   NCT04739800/logprob_matrix_8.csv  \n",
       "1498   NCT04739800/logprob_matrix_9.csv  \n",
       "\n",
       "[1499 rows x 683 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39c0e1-27d4-41ff-98d7-26cf05f7115e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa-memo",
   "language": "python",
   "name": "fa-memo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
