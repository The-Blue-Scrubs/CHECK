{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a0207-4cd6-4782-b568-5d972d2f2cfb",
   "metadata": {},
   "source": [
    "## Here we reduce and concatenate the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5445748e-6fea-4172-82a7-c55c5867bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 CSV files to process\n",
      "Output will have 681 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:52<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 CSV files to concatenate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:10<00:00, 141.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame info:\n",
      "Shape: (1500, 683)\n",
      "Number of samples: 1500\n",
      "Number of features: 683\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "fact             1460\n",
      "hallucination      34\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_fixed_column_names():\n",
    "    \"\"\"\n",
    "    Generate a fixed list of column names to ensure consistency across all files\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    \n",
    "    # Models (excluding model_5)\n",
    "    models = [f\"model_{i}\" for i in range(1, 7) if i != 5]\n",
    "    \n",
    "    # Metrics for each model\n",
    "    metrics = ['gold_lp', 'rank', 'Entropy', 'gold_lp_cumsum', 'gold_prob', 'gold_lp_normalized']\n",
    "    \n",
    "    # Statistics to compute\n",
    "    stats = ['median', 'max', 'min', 'std', \n",
    "             'moment1', 'moment2', 'moment3', 'moment4', 'moment5', \n",
    "             'q95', 'q90', 'q85', 'q80', 'q20', 'q15', 'q10', 'q05']\n",
    "    \n",
    "    # Generate columns for model metrics\n",
    "    for model in models:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{model}_{metric}_{stat}\")\n",
    "    \n",
    "    # Generate columns for KL divergence terms\n",
    "    model_pairs = [(i, j) for i in range(1, 7) for j in range(i+1, 7) \n",
    "                  if i != 5 and j != 5]\n",
    "    \n",
    "    for i, j in model_pairs:\n",
    "        for stat in stats:\n",
    "            columns.append(f\"kl_{i}_vs_{j}_{stat}\")\n",
    "    \n",
    "    return columns\n",
    "\n",
    "def get_label_from_json(trial_name, question_number, label_folder_path):\n",
    "    \"\"\"\n",
    "    Get label from corresponding JSON file if label_folder_path is provided\n",
    "    \"\"\"\n",
    "    if not label_folder_path:  # If no path provided\n",
    "        return None\n",
    "        \n",
    "    json_path = os.path.join(\n",
    "        label_folder_path,\n",
    "        f\"{trial_name}.json\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            label = data[f\"Q{question_number}\"][\"eval_databases\"]\n",
    "            # Explicitly handle \"N/A\" to prevent pandas from converting it\n",
    "            if label == \"N/A\":\n",
    "                return \"N/A\"  # Forces pandas to keep it as string\n",
    "            return label\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading JSON for {trial_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_trial_and_question(file_path):\n",
    "    \"\"\"\n",
    "    Extract trial name and question number from file path\n",
    "    Example: path/to/NCT00001959/logprob_matrix_1.csv -> (\"NCT00001959\", 1)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get filename and remove extension\n",
    "        filename = os.path.basename(file_path)\n",
    "        # Extract question number\n",
    "        question_num = int(filename.split('_')[-1].split('.')[0])\n",
    "        # Extract trial name from path\n",
    "        trial_name = file_path.split('/')[-2]  # Adjust this based on your actual path structure\n",
    "        return trial_name, question_num\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting trial and question from {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def compute_statistics(series):\n",
    "    \"\"\"\n",
    "    Compute various statistical measures for a series\n",
    "    \"\"\"\n",
    "    mean = series.mean()\n",
    "    \n",
    "    stats = {\n",
    "        'median': series.median(),\n",
    "        'max': series.max(),\n",
    "        'min': series.min(),\n",
    "        'std': series.std(),\n",
    "        \n",
    "        # Central moments\n",
    "        'moment1': mean,\n",
    "        'moment2': ((series - mean) ** 2).mean(),\n",
    "        'moment3': ((series - mean) ** 3).mean(),\n",
    "        'moment4': ((series - mean) ** 4).mean(),\n",
    "        'moment5': ((series - mean) ** 5).mean(),\n",
    "        \n",
    "        # Existing quantiles\n",
    "        'q95': series.quantile(0.95),\n",
    "        'q90': series.quantile(0.90),\n",
    "        'q85': series.quantile(0.85),\n",
    "        'q80': series.quantile(0.80),\n",
    "        'q20': series.quantile(0.20),\n",
    "        'q15': series.quantile(0.15),\n",
    "        'q10': series.quantile(0.10),\n",
    "        'q05': series.quantile(0.05),\n",
    "    }    \n",
    "    return stats\n",
    "\n",
    "def process_metrics_and_kl(input_folder, kl_folder, output_folder, label_folder_path=None):\n",
    "    \"\"\"\n",
    "    Process original metrics and KL divergence terms with consistent column ordering\n",
    "    \"\"\"\n",
    "    # Get fixed column names\n",
    "    fixed_columns = get_fixed_column_names()\n",
    "    \n",
    "    # Original metrics to analyze\n",
    "    metrics = ['gold_lp', 'rank', 'Entropy', 'gold_lp_cumsum', 'gold_prob', 'gold_lp_normalized']\n",
    "    \n",
    "    # Get all CSV files from model1 directory\n",
    "    csv_files = []\n",
    "    model1_path = os.path.join(input_folder, \"model_1\")\n",
    "    for root, dirs, files in os.walk(model1_path):\n",
    "        # Skip hidden directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for file in files:\n",
    "            if not file.startswith('.') and file.endswith('.csv'):\n",
    "                csv_files.append((os.path.join(root, file), file))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    print(f\"Output will have {len(fixed_columns) + 1} columns\")  # +1 for label\n",
    "    \n",
    "    for file_path1, file_name in tqdm(csv_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            # Get trial name and question number\n",
    "            trial_name, question_num = extract_trial_and_question(file_path1)\n",
    "            \n",
    "            if trial_name and question_num and label_folder_path:\n",
    "                # Get label from JSON only if path is provided\n",
    "                label = get_label_from_json(trial_name, question_num, label_folder_path)\n",
    "            else:\n",
    "                label = None\n",
    "            \n",
    "            # Dictionary to store all metrics\n",
    "            all_stats = {col: np.nan for col in fixed_columns}  # Initialize with NaN\n",
    "            \n",
    "            # Add label column\n",
    "            all_stats['label'] = label\n",
    "            \n",
    "            # Process each model (excluding model_5)\n",
    "            models = [f\"model_{i}\" for i in range(1, 7) if i != 5]\n",
    "            \n",
    "            # Step 1: Process original metrics for each model\n",
    "            for model in models:\n",
    "                file_path = os.path.join(input_folder, model, os.path.relpath(file_path1, model1_path))\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"Skipping {file_name} - no matching file in {model}\")\n",
    "                    continue\n",
    "                \n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Compute statistics for each metric\n",
    "                for metric in metrics:\n",
    "                    if metric in df.columns:\n",
    "                        stats = compute_statistics(df[metric])\n",
    "                        for stat_name, value in stats.items():\n",
    "                            col_name = f\"{model}_{metric}_{stat_name}\"\n",
    "                            all_stats[col_name] = value\n",
    "            \n",
    "            # Step 2: Process KL divergence terms\n",
    "            kl_file_path = os.path.join(kl_folder, os.path.relpath(file_path1, model1_path))\n",
    "            if os.path.exists(kl_file_path):\n",
    "                kl_df = pd.read_csv(kl_file_path)\n",
    "                \n",
    "                # Get all KL columns\n",
    "                kl_cols = [col for col in kl_df.columns if col.startswith('kl_')]\n",
    "                \n",
    "                # Compute statistics for each KL term\n",
    "                for kl_col in kl_cols:\n",
    "                    stats = compute_statistics(kl_df[kl_col])\n",
    "                    for stat_name, value in stats.items():\n",
    "                        col_name = f\"{kl_col}_{stat_name}\"\n",
    "                        all_stats[col_name] = value\n",
    "            \n",
    "            # Create output DataFrame with fixed column order plus label\n",
    "            columns_with_label = fixed_columns + ['label']\n",
    "            result_df = pd.DataFrame([all_stats])[columns_with_label]\n",
    "            \n",
    "            # Create output directory structure\n",
    "            output_file_path = os.path.join(output_folder, os.path.relpath(file_path1, model1_path))\n",
    "            os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "            \n",
    "            # Save results\n",
    "            result_df.to_csv(output_file_path, index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {file_name}:\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "def concatenate_output_files(output_folder):\n",
    "    \"\"\"\n",
    "    Concatenate all CSV files in Output_folder and its subfolders into a single DataFrame\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(output_folder):\n",
    "        # Skip hidden directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for file in files:\n",
    "            if not file.startswith('.') and file.endswith('.csv'):\n",
    "                csv_files.append((os.path.join(root, file), file))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to concatenate\")\n",
    "    # Sort files alphabetically\n",
    "    csv_files = sorted(csv_files)\n",
    "    \n",
    "    # Read and concatenate all files\n",
    "    all_dfs = []\n",
    "    for file_path, file_name in tqdm(csv_files, desc=\"Reading files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Add filename as a column (optional but useful for tracking)\n",
    "            df['source_file'] = file_name\n",
    "            \n",
    "            # Add full path as a column (optional)\n",
    "            df['file_path'] = os.path.relpath(file_path, output_folder)\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError reading {file_name}:\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        # Print some information about the final DataFrame\n",
    "        print(\"\\nFinal DataFrame info:\")\n",
    "        print(f\"Shape: {final_df.shape}\")\n",
    "        print(f\"Number of samples: {len(final_df)}\")\n",
    "        print(f\"Number of features: {len(final_df.columns)}\")\n",
    "        \n",
    "        if 'label' in final_df.columns:\n",
    "            print(\"\\nLabel distribution:\")\n",
    "            print(final_df['label'].value_counts())\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No files were successfully read!\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    input_folder = \"Database_free_evaluation/Clinical_trials/Features/FEATURES_Llama3.3-70B/Paragraph_summary_features\"\n",
    "    kl_folder = \"kl_analysis_Paragraph_summary_features\"\n",
    "    output_folder = \"Output_folder_Paragraph_summary\"\n",
    "\n",
    "     # Label folder path (set to None if not using labels)\n",
    "    label_folder_path = \"Database_dependent_evaluation/Clinical_trials/4-Evaluation/Evaluation_factual/Llama3.3-70B/Paragraph_level/Evaluation_summary_Paragraph\"\n",
    "    # Or for other folders:\n",
    "    # label_folder_path = None\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Step 1: Process files and create individual outputs\n",
    "    process_metrics_and_kl(input_folder, kl_folder, output_folder, label_folder_path)\n",
    "\n",
    "    # Step 2: Concatenate all output files\n",
    "    final_df = concatenate_output_files(output_folder)\n",
    "    \n",
    "    # Optional: Save concatenated DataFrame\n",
    "    if final_df is not None:\n",
    "        final_df.to_csv(\"concatenated_results_Paragraph_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8f55d-5f71-4255-b188-02cbed45b642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccab0e6f-224e-483e-b3d5-967756182838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_1_gold_lp_median</th>\n",
       "      <th>model_1_gold_lp_max</th>\n",
       "      <th>model_1_gold_lp_min</th>\n",
       "      <th>model_1_gold_lp_std</th>\n",
       "      <th>model_1_gold_lp_moment1</th>\n",
       "      <th>model_1_gold_lp_moment2</th>\n",
       "      <th>model_1_gold_lp_moment3</th>\n",
       "      <th>model_1_gold_lp_moment4</th>\n",
       "      <th>model_1_gold_lp_moment5</th>\n",
       "      <th>model_1_gold_lp_q95</th>\n",
       "      <th>...</th>\n",
       "      <th>kl_4_vs_6_q90</th>\n",
       "      <th>kl_4_vs_6_q85</th>\n",
       "      <th>kl_4_vs_6_q80</th>\n",
       "      <th>kl_4_vs_6_q20</th>\n",
       "      <th>kl_4_vs_6_q15</th>\n",
       "      <th>kl_4_vs_6_q10</th>\n",
       "      <th>kl_4_vs_6_q05</th>\n",
       "      <th>label</th>\n",
       "      <th>source_file</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.469582</td>\n",
       "      <td>0.062336</td>\n",
       "      <td>-0.009575</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.000355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229603</td>\n",
       "      <td>0.039533</td>\n",
       "      <td>0.019408</td>\n",
       "      <td>6.957359e-07</td>\n",
       "      <td>3.180618e-07</td>\n",
       "      <td>2.052192e-07</td>\n",
       "      <td>1.287433e-08</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_1.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.980232e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.196398</td>\n",
       "      <td>2.163222</td>\n",
       "      <td>-0.524352</td>\n",
       "      <td>4.512401</td>\n",
       "      <td>-43.779671</td>\n",
       "      <td>464.517646</td>\n",
       "      <td>-4946.832018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420665</td>\n",
       "      <td>0.311356</td>\n",
       "      <td>0.254785</td>\n",
       "      <td>7.169600e-05</td>\n",
       "      <td>2.824224e-05</td>\n",
       "      <td>1.488498e-05</td>\n",
       "      <td>3.977073e-06</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_10.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_10.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.318674</td>\n",
       "      <td>0.358092</td>\n",
       "      <td>-0.183229</td>\n",
       "      <td>0.119681</td>\n",
       "      <td>-0.096271</td>\n",
       "      <td>0.112248</td>\n",
       "      <td>-0.125854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949250</td>\n",
       "      <td>0.866495</td>\n",
       "      <td>0.536783</td>\n",
       "      <td>1.409192e-04</td>\n",
       "      <td>1.123663e-04</td>\n",
       "      <td>8.786914e-05</td>\n",
       "      <td>7.300707e-05</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_11.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_11.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.702140</td>\n",
       "      <td>0.904958</td>\n",
       "      <td>-0.141555</td>\n",
       "      <td>0.807731</td>\n",
       "      <td>-5.921203</td>\n",
       "      <td>44.762865</td>\n",
       "      <td>-338.419368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379328</td>\n",
       "      <td>0.265153</td>\n",
       "      <td>0.195337</td>\n",
       "      <td>1.136319e-06</td>\n",
       "      <td>6.168359e-07</td>\n",
       "      <td>2.115952e-07</td>\n",
       "      <td>2.655602e-08</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_12.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_12.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.513278e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.755051</td>\n",
       "      <td>0.421117</td>\n",
       "      <td>-0.165742</td>\n",
       "      <td>0.176993</td>\n",
       "      <td>-0.619950</td>\n",
       "      <td>3.746766</td>\n",
       "      <td>-24.333245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287349</td>\n",
       "      <td>0.229822</td>\n",
       "      <td>0.163484</td>\n",
       "      <td>6.523635e-05</td>\n",
       "      <td>1.996456e-05</td>\n",
       "      <td>7.435319e-06</td>\n",
       "      <td>2.005330e-06</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_13.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_13.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-1.876774e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.402549</td>\n",
       "      <td>0.386656</td>\n",
       "      <td>-0.136843</td>\n",
       "      <td>0.149211</td>\n",
       "      <td>-0.386540</td>\n",
       "      <td>1.657368</td>\n",
       "      <td>-8.167006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.330748</td>\n",
       "      <td>0.249763</td>\n",
       "      <td>1.441484e-04</td>\n",
       "      <td>4.974966e-05</td>\n",
       "      <td>5.792682e-06</td>\n",
       "      <td>6.281551e-07</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_5.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_5.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>-1.329166e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.257800</td>\n",
       "      <td>3.241725</td>\n",
       "      <td>-1.031837</td>\n",
       "      <td>9.457901</td>\n",
       "      <td>-77.559497</td>\n",
       "      <td>725.509797</td>\n",
       "      <td>-6683.318437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.453868</td>\n",
       "      <td>1.421968</td>\n",
       "      <td>1.421351</td>\n",
       "      <td>1.745008e-04</td>\n",
       "      <td>8.185911e-05</td>\n",
       "      <td>9.656293e-06</td>\n",
       "      <td>8.989429e-06</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_6.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_6.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.091068</td>\n",
       "      <td>1.194335</td>\n",
       "      <td>-0.253525</td>\n",
       "      <td>1.371574</td>\n",
       "      <td>-7.636523</td>\n",
       "      <td>44.666768</td>\n",
       "      <td>-260.720646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.159506</td>\n",
       "      <td>0.721443</td>\n",
       "      <td>0.039108</td>\n",
       "      <td>5.195794e-05</td>\n",
       "      <td>2.996927e-05</td>\n",
       "      <td>1.289915e-05</td>\n",
       "      <td>6.302248e-06</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_7.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_7.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.523907</td>\n",
       "      <td>1.152263</td>\n",
       "      <td>-0.226127</td>\n",
       "      <td>1.286219</td>\n",
       "      <td>-7.795517</td>\n",
       "      <td>49.161001</td>\n",
       "      <td>-309.589898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513853</td>\n",
       "      <td>0.389395</td>\n",
       "      <td>0.119939</td>\n",
       "      <td>2.643177e-05</td>\n",
       "      <td>2.159779e-05</td>\n",
       "      <td>3.763596e-06</td>\n",
       "      <td>5.721547e-07</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_8.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_8.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.706168</td>\n",
       "      <td>0.780714</td>\n",
       "      <td>-0.091318</td>\n",
       "      <td>0.603358</td>\n",
       "      <td>-4.468792</td>\n",
       "      <td>33.972528</td>\n",
       "      <td>-258.634591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022118</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>1.562375e-06</td>\n",
       "      <td>1.185927e-06</td>\n",
       "      <td>7.479567e-07</td>\n",
       "      <td>4.035694e-07</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_9.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_9.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 683 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_1_gold_lp_median  model_1_gold_lp_max  model_1_gold_lp_min  \\\n",
       "0               0.000000e+00                  0.0            -0.469582   \n",
       "1              -2.980232e-07                  0.0           -11.196398   \n",
       "2               0.000000e+00                  0.0            -1.318674   \n",
       "3               0.000000e+00                  0.0            -7.702140   \n",
       "4              -5.513278e-05                  0.0            -6.755051   \n",
       "...                      ...                  ...                  ...   \n",
       "1495           -1.876774e-04                  0.0            -5.402549   \n",
       "1496           -1.329166e-05                  0.0           -10.257800   \n",
       "1497            0.000000e+00                  0.0            -6.091068   \n",
       "1498            0.000000e+00                  0.0            -6.523907   \n",
       "1499            0.000000e+00                  0.0            -7.706168   \n",
       "\n",
       "      model_1_gold_lp_std  model_1_gold_lp_moment1  model_1_gold_lp_moment2  \\\n",
       "0                0.062336                -0.009575                 0.003819   \n",
       "1                2.163222                -0.524352                 4.512401   \n",
       "2                0.358092                -0.183229                 0.119681   \n",
       "3                0.904958                -0.141555                 0.807731   \n",
       "4                0.421117                -0.165742                 0.176993   \n",
       "...                   ...                      ...                      ...   \n",
       "1495             0.386656                -0.136843                 0.149211   \n",
       "1496             3.241725                -1.031837                 9.457901   \n",
       "1497             1.194335                -0.253525                 1.371574   \n",
       "1498             1.152263                -0.226127                 1.286219   \n",
       "1499             0.780714                -0.091318                 0.603358   \n",
       "\n",
       "      model_1_gold_lp_moment3  model_1_gold_lp_moment4  \\\n",
       "0                   -0.001683                 0.000772   \n",
       "1                  -43.779671               464.517646   \n",
       "2                   -0.096271                 0.112248   \n",
       "3                   -5.921203                44.762865   \n",
       "4                   -0.619950                 3.746766   \n",
       "...                       ...                      ...   \n",
       "1495                -0.386540                 1.657368   \n",
       "1496               -77.559497               725.509797   \n",
       "1497                -7.636523                44.666768   \n",
       "1498                -7.795517                49.161001   \n",
       "1499                -4.468792                33.972528   \n",
       "\n",
       "      model_1_gold_lp_moment5  model_1_gold_lp_q95  ...  kl_4_vs_6_q90  \\\n",
       "0                   -0.000355                  0.0  ...       0.229603   \n",
       "1                -4946.832018                  0.0  ...       0.420665   \n",
       "2                   -0.125854                  0.0  ...       0.949250   \n",
       "3                 -338.419368                  0.0  ...       0.379328   \n",
       "4                  -24.333245                  0.0  ...       0.287349   \n",
       "...                       ...                  ...  ...            ...   \n",
       "1495                -8.167006                  0.0  ...       0.464387   \n",
       "1496             -6683.318437                  0.0  ...       1.453868   \n",
       "1497              -260.720646                  0.0  ...       1.159506   \n",
       "1498              -309.589898                  0.0  ...       0.513853   \n",
       "1499              -258.634591                  0.0  ...       0.022118   \n",
       "\n",
       "      kl_4_vs_6_q85  kl_4_vs_6_q80  kl_4_vs_6_q20  kl_4_vs_6_q15  \\\n",
       "0          0.039533       0.019408   6.957359e-07   3.180618e-07   \n",
       "1          0.311356       0.254785   7.169600e-05   2.824224e-05   \n",
       "2          0.866495       0.536783   1.409192e-04   1.123663e-04   \n",
       "3          0.265153       0.195337   1.136319e-06   6.168359e-07   \n",
       "4          0.229822       0.163484   6.523635e-05   1.996456e-05   \n",
       "...             ...            ...            ...            ...   \n",
       "1495       0.330748       0.249763   1.441484e-04   4.974966e-05   \n",
       "1496       1.421968       1.421351   1.745008e-04   8.185911e-05   \n",
       "1497       0.721443       0.039108   5.195794e-05   2.996927e-05   \n",
       "1498       0.389395       0.119939   2.643177e-05   2.159779e-05   \n",
       "1499       0.003992       0.000720   1.562375e-06   1.185927e-06   \n",
       "\n",
       "      kl_4_vs_6_q10  kl_4_vs_6_q05  label            source_file  \\\n",
       "0      2.052192e-07   1.287433e-08   fact   logprob_matrix_1.csv   \n",
       "1      1.488498e-05   3.977073e-06   fact  logprob_matrix_10.csv   \n",
       "2      8.786914e-05   7.300707e-05   fact  logprob_matrix_11.csv   \n",
       "3      2.115952e-07   2.655602e-08   fact  logprob_matrix_12.csv   \n",
       "4      7.435319e-06   2.005330e-06   fact  logprob_matrix_13.csv   \n",
       "...             ...            ...    ...                    ...   \n",
       "1495   5.792682e-06   6.281551e-07   fact   logprob_matrix_5.csv   \n",
       "1496   9.656293e-06   8.989429e-06   fact   logprob_matrix_6.csv   \n",
       "1497   1.289915e-05   6.302248e-06   fact   logprob_matrix_7.csv   \n",
       "1498   3.763596e-06   5.721547e-07   fact   logprob_matrix_8.csv   \n",
       "1499   7.479567e-07   4.035694e-07   fact   logprob_matrix_9.csv   \n",
       "\n",
       "                              file_path  \n",
       "0      NCT00001959/logprob_matrix_1.csv  \n",
       "1     NCT00001959/logprob_matrix_10.csv  \n",
       "2     NCT00001959/logprob_matrix_11.csv  \n",
       "3     NCT00001959/logprob_matrix_12.csv  \n",
       "4     NCT00001959/logprob_matrix_13.csv  \n",
       "...                                 ...  \n",
       "1495   NCT04739800/logprob_matrix_5.csv  \n",
       "1496   NCT04739800/logprob_matrix_6.csv  \n",
       "1497   NCT04739800/logprob_matrix_7.csv  \n",
       "1498   NCT04739800/logprob_matrix_8.csv  \n",
       "1499   NCT04739800/logprob_matrix_9.csv  \n",
       "\n",
       "[1500 rows x 683 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffa6459-5ea3-4c9a-b7f0-b7119a5c3151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-63.746"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "row = [-6.88, -100, -11.85, -100, -100]\n",
    "mean= np.mean(row)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ffa3dd3-2a13-4211-80b9-0eb3922d9644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.4297069988088"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = np.std(row)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9bb5f3a-597e-4c4c-8240-c01efc3fc0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2799094083948073"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-6.88 - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639d100f-e1e9-4d00-82a4-041a86c07949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -63.746\n",
      "Std: 44.4297069988088\n",
      "\n",
      "Original value: -6.88\n",
      "Normalized value: 1.2799094083948073\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "row = [-6.88, -100, -11.85, -100, -100]  # Fixed the typo in -11,85\n",
    "\n",
    "# Calculate mean and std of the vector\n",
    "mean = np.mean(row)\n",
    "std = np.std(row)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n",
    "\n",
    "# Normalize the first component\n",
    "normalized_value = (row[0] - mean) / std\n",
    "\n",
    "print(f\"\\nOriginal value: {row[0]}\")\n",
    "print(f\"Normalized value: {normalized_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39c0e1-27d4-41ff-98d7-26cf05f7115e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
