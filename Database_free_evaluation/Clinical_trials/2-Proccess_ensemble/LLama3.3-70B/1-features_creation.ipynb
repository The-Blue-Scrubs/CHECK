{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6f8ec3-6bb6-4684-89f0-2da11c6160c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 236760 CSV files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 236760/236760 [1:29:11<00:00, 44.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Successfully processed: 236760 files\n",
      "Failed to process: 0 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_entropy_from_logprobs(row, lp_cols):\n",
    "    \"\"\"\n",
    "    Given a row with columns lp_1..lp_50 (log probabilities),\n",
    "    convert them to probabilities, normalize, and compute\n",
    "    the entropy (in nats).\n",
    "    \"\"\"\n",
    "    logps = row[lp_cols].values.astype(float)\n",
    "    ps = np.exp(logps)\n",
    "    p_sum = np.sum(ps)\n",
    "    \n",
    "    if p_sum <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    p_norm = ps / p_sum\n",
    "    epsilon = 1e-12\n",
    "    entropy = -np.sum(p_norm * np.log(p_norm + epsilon))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def get_logprob_stats_across_models(csv_path):\n",
    "    \"\"\"\n",
    "    Get mean and std of gold_lp across all models for each row\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract components from the path\n",
    "        # For path: \".../FEATURES/Paragraph_title_features/model_1/NCT00001959/logprob_matrix_1.csv\"\n",
    "        parts = csv_path.split('/')\n",
    "        model_idx = parts.index([p for p in parts if p.startswith('model_')][0])\n",
    "        \n",
    "        # Reconstruct base path (up to FEATURES/Paragraph_title_features)\n",
    "        base_path = '/'.join(parts[:model_idx])\n",
    "        \n",
    "        # Get trial folder and filename\n",
    "        trial_folder = parts[model_idx + 1]\n",
    "        filename = parts[-1]\n",
    "        \n",
    "        # First, read all dataframes\n",
    "        dfs = {}\n",
    "        for model_num in [1,2,3,4,6]:\n",
    "            model_path = os.path.join(base_path, f\"model_{model_num}\", trial_folder, filename)\n",
    "            if os.path.exists(model_path):\n",
    "                dfs[f\"model_{model_num}\"] = pd.read_csv(model_path)\n",
    "        \n",
    "        if not dfs:\n",
    "            print(f\"No model files found for {csv_path}\")  # Debug print\n",
    "            return None, None\n",
    "            \n",
    "        # Get number of rows from first dataframe\n",
    "        n_rows = next(iter(dfs.values())).shape[0]\n",
    "        \n",
    "        # Initialize arrays for means and stds\n",
    "        means = np.zeros(n_rows)\n",
    "        stds = np.zeros(n_rows)\n",
    "        \n",
    "        # Calculate mean and std for each row\n",
    "        for row_idx in range(n_rows):\n",
    "            row_values = []\n",
    "            for df in dfs.values():\n",
    "                if 'gold_lp' in df.columns:\n",
    "                    row_values.append(df.iloc[row_idx]['gold_lp'])\n",
    "            \n",
    "            means[row_idx] = np.mean(row_values)\n",
    "            stds[row_idx] = np.std(row_values)\n",
    "            \n",
    "        \n",
    "        return means, stds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting logprob stats for {csv_path}: {e}\")\n",
    "        print(f\"Full path: {csv_path}\")  # Debug print\n",
    "        return None, None\n",
    "\n",
    "def add_columns(csv_file):\n",
    "    \"\"\"\n",
    "    Add entropy and gold_lp_cumsum columns to a CSV file, updating the original file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        epsilon = 1e-12\n",
    "        \n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Check if columns already exist\n",
    "        new_columns = []\n",
    "        if 'Entropy' not in df.columns:\n",
    "            new_columns.append('Entropy')\n",
    "            # Add Entropy column\n",
    "            lp_cols = [f\"lp_{i}\" for i in range(1, 51)]\n",
    "            df[\"Entropy\"] = df.apply(lambda row: compute_entropy_from_logprobs(row, lp_cols), axis=1)\n",
    "        \n",
    "        if 'gold_prob' not in df.columns or 'gold_lp_cumsum' not in df.columns:  # Changed name in check\n",
    "            new_columns.extend(['gold_prob', 'gold_lp_cumsum'])  # Changed name in new columns\n",
    "            # Convert log probabilities to probabilities\n",
    "            df[\"gold_prob\"] = np.exp(df[\"gold_lp\"].astype(float))\n",
    "            # Compute cumulative sum of probabilities\n",
    "            cumsum_probs = df[\"gold_prob\"].cumsum()\n",
    "            # Take log of cumulative sum (adding small epsilon to avoid log(0))\n",
    "            epsilon = 1e-12\n",
    "            df[\"gold_lp_cumsum\"] = np.log(cumsum_probs + epsilon)  # Changed column name\n",
    "\n",
    "        # df = df.drop('gold_lp_normalized', axis=1)\n",
    "        # Add normalized gold_lp if needed\n",
    "        if 'gold_lp_normalized' not in df.columns:\n",
    "            new_columns.append('gold_lp_normalized')\n",
    "    \n",
    "            # Get means and stds for each row\n",
    "            means, stds = get_logprob_stats_across_models(csv_file)\n",
    "    \n",
    "            if means is not None and stds is not None:\n",
    "                df['gold_lp_normalized'] = (df['gold_lp'] - means) / (stds + epsilon)\n",
    "            else:\n",
    "                print(f\"Could not compute normalization stats for {csv_file}\")\n",
    "                df['gold_lp_normalized'] = np.nan\n",
    "        \n",
    "        if new_columns:\n",
    "            # Save back to the same file only if changes were made\n",
    "            df.to_csv(csv_file, index=False)\n",
    "            # print(f\"Added {', '.join(new_columns)} to {csv_file}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"All columns already exist in {csv_file}\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_file}: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_all_csv_files(base_folder):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the base folder and its subfolders\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        # Skip hidden directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for file in files:\n",
    "            if not file.startswith('.') and file.endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Sort files alphabetically\n",
    "    csv_files = sorted(csv_files)\n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    # Process each file\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    with tqdm(total=len(csv_files), desc=\"Processing CSV files\") as pbar:\n",
    "        for csv_file in csv_files:\n",
    "            # Process the file\n",
    "            if add_columns(csv_file):\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Successfully processed: {successful} files\")\n",
    "    print(f\"Failed to process: {failed} files\")\n",
    "    \n",
    "    return successful, failed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all CSV files in Paragraph_title folder and its subfolders\n",
    "    base_folder = \"Database_free_evaluation/Clinical_trials/Features/FEATURES_Llama3.3-70B/Paragraph_title_features\"\n",
    "    # base_folder = \"Database_free_evaluation/Clinical_trials/Features/FEATURES_Llama3.3-70B/Paragraph_summary_features\"\n",
    "    \n",
    "    successful, failed = process_all_csv_files(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfadc0c-9ecc-40f2-86c7-a8dc59f62a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa-memo",
   "language": "python",
   "name": "fa-memo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
