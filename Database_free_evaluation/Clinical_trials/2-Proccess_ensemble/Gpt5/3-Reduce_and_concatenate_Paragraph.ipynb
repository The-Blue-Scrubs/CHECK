{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a0207-4cd6-4782-b568-5d972d2f2cfb",
   "metadata": {},
   "source": [
    "## Here we reduce and concatenate the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5445748e-6fea-4172-82a7-c55c5867bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 CSV files to process\n",
      "Output will have 681 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:44<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 CSV files to concatenate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:10<00:00, 140.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame info:\n",
      "Shape: (1500, 683)\n",
      "Number of samples: 1500\n",
      "Number of features: 683\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "hallucination    957\n",
      "fact             540\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_fixed_column_names():\n",
    "    \"\"\"\n",
    "    Generate a fixed list of column names to ensure consistency across all files\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    \n",
    "    # Models (excluding model_5)\n",
    "    models = [f\"model_{i}\" for i in range(1, 7) if i != 5]\n",
    "    \n",
    "    # Metrics for each model\n",
    "    metrics = ['gold_lp', 'rank', 'Entropy', 'gold_lp_cumsum', 'gold_prob', 'gold_lp_normalized']\n",
    "    \n",
    "    # Statistics to compute\n",
    "    stats = ['median', 'max', 'min', 'std', \n",
    "             'moment1', 'moment2', 'moment3', 'moment4', 'moment5', \n",
    "             'q95', 'q90', 'q85', 'q80', 'q20', 'q15', 'q10', 'q05']\n",
    "    \n",
    "    # Generate columns for model metrics\n",
    "    for model in models:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{model}_{metric}_{stat}\")\n",
    "    \n",
    "    # Generate columns for KL divergence terms\n",
    "    model_pairs = [(i, j) for i in range(1, 7) for j in range(i+1, 7) \n",
    "                  if i != 5 and j != 5]\n",
    "    \n",
    "    for i, j in model_pairs:\n",
    "        for stat in stats:\n",
    "            columns.append(f\"kl_{i}_vs_{j}_{stat}\")\n",
    "    \n",
    "    return columns\n",
    "\n",
    "def get_label_from_json(trial_name, question_number, label_folder_path):\n",
    "    \"\"\"\n",
    "    Get label from corresponding JSON file if label_folder_path is provided\n",
    "    \"\"\"\n",
    "    if not label_folder_path:  # If no path provided\n",
    "        return None\n",
    "        \n",
    "    json_path = os.path.join(\n",
    "        label_folder_path,\n",
    "        f\"{trial_name}.json\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            label = data[f\"Q{question_number}\"][\"eval_databases\"]\n",
    "            # Explicitly handle \"N/A\" to prevent pandas from converting it\n",
    "            if label == \"N/A\":\n",
    "                return \"N/A\"  # Forces pandas to keep it as string\n",
    "            return label\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading JSON for {trial_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_trial_and_question(file_path):\n",
    "    \"\"\"\n",
    "    Extract trial name and question number from file path\n",
    "    Example: path/to/NCT00001959/logprob_matrix_1.csv -> (\"NCT00001959\", 1)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get filename and remove extension\n",
    "        filename = os.path.basename(file_path)\n",
    "        # Extract question number\n",
    "        question_num = int(filename.split('_')[-1].split('.')[0])\n",
    "        # Extract trial name from path\n",
    "        trial_name = file_path.split('/')[-2]  # Adjust this based on your actual path structure\n",
    "        return trial_name, question_num\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting trial and question from {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def compute_statistics(series):\n",
    "    \"\"\"\n",
    "    Compute various statistical measures for a series\n",
    "    \"\"\"\n",
    "    mean = series.mean()\n",
    "    \n",
    "    stats = {\n",
    "        'median': series.median(),\n",
    "        'max': series.max(),\n",
    "        'min': series.min(),\n",
    "        'std': series.std(),\n",
    "        \n",
    "        # Central moments\n",
    "        'moment1': mean,\n",
    "        'moment2': ((series - mean) ** 2).mean(),\n",
    "        'moment3': ((series - mean) ** 3).mean(),\n",
    "        'moment4': ((series - mean) ** 4).mean(),\n",
    "        'moment5': ((series - mean) ** 5).mean(),\n",
    "        \n",
    "        # Existing quantiles\n",
    "        'q95': series.quantile(0.95),\n",
    "        'q90': series.quantile(0.90),\n",
    "        'q85': series.quantile(0.85),\n",
    "        'q80': series.quantile(0.80),\n",
    "        'q20': series.quantile(0.20),\n",
    "        'q15': series.quantile(0.15),\n",
    "        'q10': series.quantile(0.10),\n",
    "        'q05': series.quantile(0.05),\n",
    "    }    \n",
    "    return stats\n",
    "\n",
    "def process_metrics_and_kl(input_folder, kl_folder, output_folder, label_folder_path=None):\n",
    "    \"\"\"\n",
    "    Process original metrics and KL divergence terms with consistent column ordering\n",
    "    \"\"\"\n",
    "    # Get fixed column names\n",
    "    fixed_columns = get_fixed_column_names()\n",
    "    \n",
    "    # Original metrics to analyze\n",
    "    metrics = ['gold_lp', 'rank', 'Entropy', 'gold_lp_cumsum', 'gold_prob', 'gold_lp_normalized']\n",
    "    \n",
    "    # Get all CSV files from model1 directory\n",
    "    csv_files = []\n",
    "    model1_path = os.path.join(input_folder, \"model_1\")\n",
    "    for root, dirs, files in os.walk(model1_path):\n",
    "        # Skip hidden directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for file in files:\n",
    "            if not file.startswith('.') and file.endswith('.csv'):\n",
    "                csv_files.append((os.path.join(root, file), file))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    print(f\"Output will have {len(fixed_columns) + 1} columns\")  # +1 for label\n",
    "    \n",
    "    for file_path1, file_name in tqdm(csv_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            # Get trial name and question number\n",
    "            trial_name, question_num = extract_trial_and_question(file_path1)\n",
    "            \n",
    "            if trial_name and question_num and label_folder_path:\n",
    "                # Get label from JSON only if path is provided\n",
    "                label = get_label_from_json(trial_name, question_num, label_folder_path)\n",
    "            else:\n",
    "                label = None\n",
    "            \n",
    "            # Dictionary to store all metrics\n",
    "            all_stats = {col: np.nan for col in fixed_columns}  # Initialize with NaN\n",
    "            \n",
    "            # Add label column\n",
    "            all_stats['label'] = label\n",
    "            \n",
    "            # Process each model (excluding model_5)\n",
    "            models = [f\"model_{i}\" for i in range(1, 7) if i != 5]\n",
    "            \n",
    "            # Step 1: Process original metrics for each model\n",
    "            for model in models:\n",
    "                file_path = os.path.join(input_folder, model, os.path.relpath(file_path1, model1_path))\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"Skipping {file_name} - no matching file in {model}\")\n",
    "                    continue\n",
    "                \n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Compute statistics for each metric\n",
    "                for metric in metrics:\n",
    "                    if metric in df.columns:\n",
    "                        stats = compute_statistics(df[metric])\n",
    "                        for stat_name, value in stats.items():\n",
    "                            col_name = f\"{model}_{metric}_{stat_name}\"\n",
    "                            all_stats[col_name] = value\n",
    "            \n",
    "            # Step 2: Process KL divergence terms\n",
    "            kl_file_path = os.path.join(kl_folder, os.path.relpath(file_path1, model1_path))\n",
    "            if os.path.exists(kl_file_path):\n",
    "                kl_df = pd.read_csv(kl_file_path)\n",
    "                \n",
    "                # Get all KL columns\n",
    "                kl_cols = [col for col in kl_df.columns if col.startswith('kl_')]\n",
    "                \n",
    "                # Compute statistics for each KL term\n",
    "                for kl_col in kl_cols:\n",
    "                    stats = compute_statistics(kl_df[kl_col])\n",
    "                    for stat_name, value in stats.items():\n",
    "                        col_name = f\"{kl_col}_{stat_name}\"\n",
    "                        all_stats[col_name] = value\n",
    "            \n",
    "            # Create output DataFrame with fixed column order plus label\n",
    "            columns_with_label = fixed_columns + ['label']\n",
    "            result_df = pd.DataFrame([all_stats])[columns_with_label]\n",
    "            \n",
    "            # Create output directory structure\n",
    "            output_file_path = os.path.join(output_folder, os.path.relpath(file_path1, model1_path))\n",
    "            os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "            \n",
    "            # Save results\n",
    "            result_df.to_csv(output_file_path, index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {file_name}:\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "def concatenate_output_files(output_folder):\n",
    "    \"\"\"\n",
    "    Concatenate all CSV files in Output_folder and its subfolders into a single DataFrame\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(output_folder):\n",
    "        # Skip hidden directories\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for file in files:\n",
    "            if not file.startswith('.') and file.endswith('.csv'):\n",
    "                csv_files.append((os.path.join(root, file), file))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to concatenate\")\n",
    "    # Sort files alphabetically\n",
    "    csv_files = sorted(csv_files)\n",
    "    \n",
    "    # Read and concatenate all files\n",
    "    all_dfs = []\n",
    "    for file_path, file_name in tqdm(csv_files, desc=\"Reading files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Add filename as a column (optional but useful for tracking)\n",
    "            df['source_file'] = file_name\n",
    "            \n",
    "            # Add full path as a column (optional)\n",
    "            df['file_path'] = os.path.relpath(file_path, output_folder)\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError reading {file_name}:\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        # Print some information about the final DataFrame\n",
    "        print(\"\\nFinal DataFrame info:\")\n",
    "        print(f\"Shape: {final_df.shape}\")\n",
    "        print(f\"Number of samples: {len(final_df)}\")\n",
    "        print(f\"Number of features: {len(final_df.columns)}\")\n",
    "        \n",
    "        if 'label' in final_df.columns:\n",
    "            print(\"\\nLabel distribution:\")\n",
    "            print(final_df['label'].value_counts())\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No files were successfully read!\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    input_folder = \"Database_free_evaluation/Clinical_trials/Features/FEATURES_Gpt5/Paragraph_title_features\"\n",
    "    kl_folder = \"kl_analysis_Paragraph_title_features\"\n",
    "    output_folder = \"Output_folder_Paragraph_title\"\n",
    "\n",
    "     # Label folder path (set to None if not using labels)\n",
    "    label_folder_path = \"Database_dependent_evaluation/Clinical_trials/4-Evaluation/Evaluation_factual/Gpt5/Paragraph_level/Evaluation_title_Paragraph_GPT\"\n",
    "    # Or for other folders:\n",
    "    # label_folder_path = None\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Step 1: Process files and create individual outputs\n",
    "    process_metrics_and_kl(input_folder, kl_folder, output_folder, label_folder_path)\n",
    "\n",
    "    # Step 2: Concatenate all output files\n",
    "    final_df = concatenate_output_files(output_folder)\n",
    "    \n",
    "    # Optional: Save concatenated DataFrame\n",
    "    if final_df is not None:\n",
    "        final_df.to_csv(\"concatenated_results_Paragraph_title_Gpt5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8f55d-5f71-4255-b188-02cbed45b642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccab0e6f-224e-483e-b3d5-967756182838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_1_gold_lp_median</th>\n",
       "      <th>model_1_gold_lp_max</th>\n",
       "      <th>model_1_gold_lp_min</th>\n",
       "      <th>model_1_gold_lp_std</th>\n",
       "      <th>model_1_gold_lp_moment1</th>\n",
       "      <th>model_1_gold_lp_moment2</th>\n",
       "      <th>model_1_gold_lp_moment3</th>\n",
       "      <th>model_1_gold_lp_moment4</th>\n",
       "      <th>model_1_gold_lp_moment5</th>\n",
       "      <th>model_1_gold_lp_q95</th>\n",
       "      <th>...</th>\n",
       "      <th>kl_4_vs_6_q90</th>\n",
       "      <th>kl_4_vs_6_q85</th>\n",
       "      <th>kl_4_vs_6_q80</th>\n",
       "      <th>kl_4_vs_6_q20</th>\n",
       "      <th>kl_4_vs_6_q15</th>\n",
       "      <th>kl_4_vs_6_q10</th>\n",
       "      <th>kl_4_vs_6_q05</th>\n",
       "      <th>label</th>\n",
       "      <th>source_file</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.002555</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-19.942202</td>\n",
       "      <td>3.237032</td>\n",
       "      <td>-1.324681</td>\n",
       "      <td>10.328688</td>\n",
       "      <td>-117.663115</td>\n",
       "      <td>1.916203e+03</td>\n",
       "      <td>-3.349904e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322478</td>\n",
       "      <td>0.273671</td>\n",
       "      <td>0.198252</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.673825e-07</td>\n",
       "      <td>1.383371e-07</td>\n",
       "      <td>5.730916e-08</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_1.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.443523</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>17.210684</td>\n",
       "      <td>-5.278262</td>\n",
       "      <td>293.963631</td>\n",
       "      <td>-25714.653499</td>\n",
       "      <td>2.440374e+06</td>\n",
       "      <td>-2.310694e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371619</td>\n",
       "      <td>0.290020</td>\n",
       "      <td>0.237494</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>3.067300e-05</td>\n",
       "      <td>7.372770e-06</td>\n",
       "      <td>5.163657e-07</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_10.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_10.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.277729</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>19.870327</td>\n",
       "      <td>-5.677067</td>\n",
       "      <td>392.479726</td>\n",
       "      <td>-34858.320077</td>\n",
       "      <td>3.298713e+06</td>\n",
       "      <td>-3.110793e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413269</td>\n",
       "      <td>0.296209</td>\n",
       "      <td>0.254602</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>2.967417e-04</td>\n",
       "      <td>9.562714e-06</td>\n",
       "      <td>8.209919e-07</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_11.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_11.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.852039</td>\n",
       "      <td>-1.192093e-07</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>26.665244</td>\n",
       "      <td>-9.935672</td>\n",
       "      <td>699.925306</td>\n",
       "      <td>-56484.975866</td>\n",
       "      <td>5.146168e+06</td>\n",
       "      <td>-4.629185e+08</td>\n",
       "      <td>-1.646198e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546066</td>\n",
       "      <td>0.507862</td>\n",
       "      <td>0.481236</td>\n",
       "      <td>0.018862</td>\n",
       "      <td>9.324209e-03</td>\n",
       "      <td>7.637982e-04</td>\n",
       "      <td>6.056297e-05</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_12.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_12.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.072655</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>20.028006</td>\n",
       "      <td>-6.069710</td>\n",
       "      <td>396.942696</td>\n",
       "      <td>-34430.698761</td>\n",
       "      <td>3.244876e+06</td>\n",
       "      <td>-3.046625e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394189</td>\n",
       "      <td>0.335043</td>\n",
       "      <td>0.304713</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>5.952057e-06</td>\n",
       "      <td>2.207163e-06</td>\n",
       "      <td>5.268360e-08</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_13.csv</td>\n",
       "      <td>NCT00001959/logprob_matrix_13.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-0.002204</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>10.742269</td>\n",
       "      <td>-2.246543</td>\n",
       "      <td>114.762294</td>\n",
       "      <td>-10351.731022</td>\n",
       "      <td>1.005393e+06</td>\n",
       "      <td>-9.813261e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406825</td>\n",
       "      <td>0.238360</td>\n",
       "      <td>0.131957</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3.015074e-07</td>\n",
       "      <td>4.444115e-08</td>\n",
       "      <td>1.112983e-08</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_5.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_5.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>-0.155733</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>11.875579</td>\n",
       "      <td>-2.726345</td>\n",
       "      <td>140.864246</td>\n",
       "      <td>-12948.411565</td>\n",
       "      <td>1.258375e+06</td>\n",
       "      <td>-1.223799e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338628</td>\n",
       "      <td>0.266274</td>\n",
       "      <td>0.193969</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>4.219662e-05</td>\n",
       "      <td>4.860021e-06</td>\n",
       "      <td>3.436082e-07</td>\n",
       "      <td>fact</td>\n",
       "      <td>logprob_matrix_6.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_6.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>-0.021725</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-14.123719</td>\n",
       "      <td>2.870307</td>\n",
       "      <td>-1.346026</td>\n",
       "      <td>7.996347</td>\n",
       "      <td>-67.600266</td>\n",
       "      <td>8.178574e+02</td>\n",
       "      <td>-1.015044e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378751</td>\n",
       "      <td>0.168140</td>\n",
       "      <td>0.139849</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>4.234440e-05</td>\n",
       "      <td>2.071257e-05</td>\n",
       "      <td>3.723692e-06</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_7.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_7.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>-0.348750</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>16.216446</td>\n",
       "      <td>-4.577614</td>\n",
       "      <td>261.219971</td>\n",
       "      <td>-23167.683992</td>\n",
       "      <td>2.211847e+06</td>\n",
       "      <td>-2.109779e+08</td>\n",
       "      <td>-5.364418e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377780</td>\n",
       "      <td>0.235627</td>\n",
       "      <td>0.219161</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>1.695121e-04</td>\n",
       "      <td>1.990751e-05</td>\n",
       "      <td>9.854478e-07</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_8.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_8.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>-2.340177</td>\n",
       "      <td>-1.312408e-04</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>27.585266</td>\n",
       "      <td>-10.802113</td>\n",
       "      <td>729.240761</td>\n",
       "      <td>-58522.856343</td>\n",
       "      <td>5.281100e+06</td>\n",
       "      <td>-4.704766e+08</td>\n",
       "      <td>-8.574001e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360043</td>\n",
       "      <td>0.347587</td>\n",
       "      <td>0.327118</td>\n",
       "      <td>0.052215</td>\n",
       "      <td>3.579009e-02</td>\n",
       "      <td>2.232420e-02</td>\n",
       "      <td>3.954435e-03</td>\n",
       "      <td>hallucination</td>\n",
       "      <td>logprob_matrix_9.csv</td>\n",
       "      <td>NCT04739800/logprob_matrix_9.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 683 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_1_gold_lp_median  model_1_gold_lp_max  model_1_gold_lp_min  \\\n",
       "0                  -0.002555         0.000000e+00           -19.942202   \n",
       "1                  -0.443523         0.000000e+00          -100.000000   \n",
       "2                  -0.277729         0.000000e+00          -100.000000   \n",
       "3                  -0.852039        -1.192093e-07          -100.000000   \n",
       "4                  -0.072655         0.000000e+00          -100.000000   \n",
       "...                      ...                  ...                  ...   \n",
       "1495               -0.002204         0.000000e+00          -100.000000   \n",
       "1496               -0.155733         0.000000e+00          -100.000000   \n",
       "1497               -0.021725         0.000000e+00           -14.123719   \n",
       "1498               -0.348750         0.000000e+00          -100.000000   \n",
       "1499               -2.340177        -1.312408e-04          -100.000000   \n",
       "\n",
       "      model_1_gold_lp_std  model_1_gold_lp_moment1  model_1_gold_lp_moment2  \\\n",
       "0                3.237032                -1.324681                10.328688   \n",
       "1               17.210684                -5.278262               293.963631   \n",
       "2               19.870327                -5.677067               392.479726   \n",
       "3               26.665244                -9.935672               699.925306   \n",
       "4               20.028006                -6.069710               396.942696   \n",
       "...                   ...                      ...                      ...   \n",
       "1495            10.742269                -2.246543               114.762294   \n",
       "1496            11.875579                -2.726345               140.864246   \n",
       "1497             2.870307                -1.346026                 7.996347   \n",
       "1498            16.216446                -4.577614               261.219971   \n",
       "1499            27.585266               -10.802113               729.240761   \n",
       "\n",
       "      model_1_gold_lp_moment3  model_1_gold_lp_moment4  \\\n",
       "0                 -117.663115             1.916203e+03   \n",
       "1               -25714.653499             2.440374e+06   \n",
       "2               -34858.320077             3.298713e+06   \n",
       "3               -56484.975866             5.146168e+06   \n",
       "4               -34430.698761             3.244876e+06   \n",
       "...                       ...                      ...   \n",
       "1495            -10351.731022             1.005393e+06   \n",
       "1496            -12948.411565             1.258375e+06   \n",
       "1497               -67.600266             8.178574e+02   \n",
       "1498            -23167.683992             2.211847e+06   \n",
       "1499            -58522.856343             5.281100e+06   \n",
       "\n",
       "      model_1_gold_lp_moment5  model_1_gold_lp_q95  ...  kl_4_vs_6_q90  \\\n",
       "0               -3.349904e+04         0.000000e+00  ...       0.322478   \n",
       "1               -2.310694e+08         0.000000e+00  ...       0.371619   \n",
       "2               -3.110793e+08         0.000000e+00  ...       0.413269   \n",
       "3               -4.629185e+08        -1.646198e-05  ...       0.546066   \n",
       "4               -3.046625e+08         0.000000e+00  ...       0.394189   \n",
       "...                       ...                  ...  ...            ...   \n",
       "1495            -9.813261e+07         0.000000e+00  ...       0.406825   \n",
       "1496            -1.223799e+08         0.000000e+00  ...       0.338628   \n",
       "1497            -1.015044e+04         0.000000e+00  ...       0.378751   \n",
       "1498            -2.109779e+08        -5.364418e-08  ...       0.377780   \n",
       "1499            -4.704766e+08        -8.574001e-03  ...       0.360043   \n",
       "\n",
       "      kl_4_vs_6_q85  kl_4_vs_6_q80  kl_4_vs_6_q20  kl_4_vs_6_q15  \\\n",
       "0          0.273671       0.198252       0.000002   2.673825e-07   \n",
       "1          0.290020       0.237494       0.000091   3.067300e-05   \n",
       "2          0.296209       0.254602       0.002974   2.967417e-04   \n",
       "3          0.507862       0.481236       0.018862   9.324209e-03   \n",
       "4          0.335043       0.304713       0.000024   5.952057e-06   \n",
       "...             ...            ...            ...            ...   \n",
       "1495       0.238360       0.131957       0.000002   3.015074e-07   \n",
       "1496       0.266274       0.193969       0.000158   4.219662e-05   \n",
       "1497       0.168140       0.139849       0.000324   4.234440e-05   \n",
       "1498       0.235627       0.219161       0.000870   1.695121e-04   \n",
       "1499       0.347587       0.327118       0.052215   3.579009e-02   \n",
       "\n",
       "      kl_4_vs_6_q10  kl_4_vs_6_q05          label            source_file  \\\n",
       "0      1.383371e-07   5.730916e-08           fact   logprob_matrix_1.csv   \n",
       "1      7.372770e-06   5.163657e-07  hallucination  logprob_matrix_10.csv   \n",
       "2      9.562714e-06   8.209919e-07  hallucination  logprob_matrix_11.csv   \n",
       "3      7.637982e-04   6.056297e-05  hallucination  logprob_matrix_12.csv   \n",
       "4      2.207163e-06   5.268360e-08  hallucination  logprob_matrix_13.csv   \n",
       "...             ...            ...            ...                    ...   \n",
       "1495   4.444115e-08   1.112983e-08           fact   logprob_matrix_5.csv   \n",
       "1496   4.860021e-06   3.436082e-07           fact   logprob_matrix_6.csv   \n",
       "1497   2.071257e-05   3.723692e-06  hallucination   logprob_matrix_7.csv   \n",
       "1498   1.990751e-05   9.854478e-07  hallucination   logprob_matrix_8.csv   \n",
       "1499   2.232420e-02   3.954435e-03  hallucination   logprob_matrix_9.csv   \n",
       "\n",
       "                              file_path  \n",
       "0      NCT00001959/logprob_matrix_1.csv  \n",
       "1     NCT00001959/logprob_matrix_10.csv  \n",
       "2     NCT00001959/logprob_matrix_11.csv  \n",
       "3     NCT00001959/logprob_matrix_12.csv  \n",
       "4     NCT00001959/logprob_matrix_13.csv  \n",
       "...                                 ...  \n",
       "1495   NCT04739800/logprob_matrix_5.csv  \n",
       "1496   NCT04739800/logprob_matrix_6.csv  \n",
       "1497   NCT04739800/logprob_matrix_7.csv  \n",
       "1498   NCT04739800/logprob_matrix_8.csv  \n",
       "1499   NCT04739800/logprob_matrix_9.csv  \n",
       "\n",
       "[1500 rows x 683 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffa6459-5ea3-4c9a-b7f0-b7119a5c3151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-63.746"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "row = [-6.88, -100, -11.85, -100, -100]\n",
    "mean= np.mean(row)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ffa3dd3-2a13-4211-80b9-0eb3922d9644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.4297069988088"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = np.std(row)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9bb5f3a-597e-4c4c-8240-c01efc3fc0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2799094083948073"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-6.88 - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639d100f-e1e9-4d00-82a4-041a86c07949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -63.746\n",
      "Std: 44.4297069988088\n",
      "\n",
      "Original value: -6.88\n",
      "Normalized value: 1.2799094083948073\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "row = [-6.88, -100, -11.85, -100, -100]  # Fixed the typo in -11,85\n",
    "\n",
    "# Calculate mean and std of the vector\n",
    "mean = np.mean(row)\n",
    "std = np.std(row)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n",
    "\n",
    "# Normalize the first component\n",
    "normalized_value = (row[0] - mean) / std\n",
    "\n",
    "print(f\"\\nOriginal value: {row[0]}\")\n",
    "print(f\"Normalized value: {normalized_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39c0e1-27d4-41ff-98d7-26cf05f7115e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa-memo",
   "language": "python",
   "name": "fa-memo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
