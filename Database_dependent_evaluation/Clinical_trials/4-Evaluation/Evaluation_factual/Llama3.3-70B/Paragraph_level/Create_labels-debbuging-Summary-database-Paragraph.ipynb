{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bc3ad6-792a-4ade-8768-4eb30563dfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-10 11:17:29 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead. See https://pypi.org/project/pynvml for more information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import logging\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514227c-af5a-4186-99a8-f2f9964b86f5",
   "metadata": {},
   "source": [
    "## Loop over input_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6e7b8-38ea-44ff-b75d-56d75acf8332",
   "metadata": {},
   "source": [
    "## Proccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa451d1b-62e9-4879-9881-9bfc6ca7f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def log_error(error_msg, error_file=\"error_log.txt\"):\n",
    "    \"\"\"Log errors to a file with timestamp\"\"\"\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(error_file, 'a') as f:\n",
    "        f.write(f\"[{timestamp}] {error_msg}\\n\")\n",
    "\n",
    "\n",
    "def get_scores(file_path1):\n",
    "    \"\"\"\n",
    "    Get scores from a JSON files and determine if hallucination or fact.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load both JSON files\n",
    "        with open(file_path1, 'r', encoding='utf-8') as f1:\n",
    "            data1 = json.load(f1)\n",
    "            \n",
    "        # Dictionary to store results for each question\n",
    "        results = {}\n",
    "        \n",
    "        # Get scores for each question (Q1 to Q15)\n",
    "        for q in range(1, 16):\n",
    "            q_key = f\"Q{q}\"\n",
    "            \n",
    "            # Skip if question doesn't exist in either file\n",
    "            if q_key not in data1:\n",
    "                continue\n",
    "                \n",
    "            # Get scores based on granularity level\n",
    "            score1 = data1[q_key].get(\"score\")\n",
    "            score2 = data1[q_key].get(\"direct_score\")\n",
    "            \n",
    "            if score1 is not None and score2 is not None:\n",
    "                # Convert scores to float for comparison\n",
    "                score1 = float(score1)\n",
    "                score2 = float(score2)\n",
    "\n",
    "                # Check for conflicting scores\n",
    "                if score1 != score2 and (score1 in [0,1] and score2 in [0,1]):\n",
    "                    print(f\"Found conflicting scores for {q_key}: {score1} vs {score2}, using direct_score\")\n",
    "                    # Use direct_score (score2) for classification\n",
    "                    if score2 == 1:\n",
    "                        results[q_key] = \"hallucination\"\n",
    "                    else:  # score2 == 0\n",
    "                        results[q_key] = \"fact\"\n",
    "                # If no conflict, use original OR logic\n",
    "                else:\n",
    "                    if score1 == 1 or score2 == 1:\n",
    "                        results[q_key] = \"hallucination\"\n",
    "                    elif score1 == 0 or score2 == 0:\n",
    "                        results[q_key] = \"fact\"\n",
    "                    else:\n",
    "                        results[q_key] = \"N/A\"\n",
    "                        print(f\"Found N/A scores for {q_key}: {score1} vs {score2}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Warning: Missing score for {q_key} in one of the files\")\n",
    "                \n",
    "        return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing files: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_evaluation_to_file(results, target_file, output_file):\n",
    "    \"\"\"\n",
    "    Add evaluation results to target file.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary with evaluation results from compare_scores\n",
    "        target_file: Path to file where results will be added\n",
    "        output_file: Path where the new file will be saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the target file\n",
    "        with open(target_file, 'r') as f:\n",
    "            target_data = json.load(f)\n",
    "            \n",
    "        # Create new dictionary with updated structure\n",
    "        updated_data = {}\n",
    "        for q_key, text in target_data.items():\n",
    "            if q_key in results:\n",
    "                updated_data[q_key] = {\n",
    "                    \"answer\": text,\n",
    "                    \"eval_databases\": results[q_key]\n",
    "                }\n",
    "            else:\n",
    "                updated_data[q_key] = {\"answer\": text}\n",
    "                \n",
    "        # Save the updated data to new file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(updated_data, f, indent=2)\n",
    "            \n",
    "        print(f\"Successfully added evaluation results to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error adding evaluation results: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_summary_report():\n",
    "    \"\"\"\n",
    "    Create a detailed summary report of the evaluation results\n",
    "    \"\"\"\n",
    "    # Initialize summary dictionaries\n",
    "    total_summary = {\n",
    "        \"total_processed\": 0,\n",
    "        \"facts\": 0,\n",
    "        \"hallucinations\": 0,\n",
    "        \"N/A\": 0  # Make sure this is initialized\n",
    "    }\n",
    "    \n",
    "    # Initialize per-question summary for each combination\n",
    "    combination_summaries = {}\n",
    "    for granularity in granularities:\n",
    "        for input_type in input_types:\n",
    "            key = f\"{granularity}_{input_type}\"\n",
    "            combination_summaries[key] = {\n",
    "                f\"Q{i}\": {\"fact\": 0, \"hallucination\": 0, \"N/A\": 0} \n",
    "                for i in range(1, 16)\n",
    "            }\n",
    "    \n",
    "    # Process each combination\n",
    "    for granularity in granularities:\n",
    "        for input_type in input_types:\n",
    "            output_dir = os.path.join(\n",
    "                output_base_path,\n",
    "                f\"{granularity}_level\",\n",
    "                f\"Inference_{input_type}_{granularity}\"\n",
    "            )\n",
    "            \n",
    "            if not os.path.exists(output_dir):\n",
    "                continue\n",
    "                \n",
    "            # Process each file in the directory\n",
    "            json_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]\n",
    "            for json_file in json_files:\n",
    "                file_path = os.path.join(output_dir, json_file)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        \n",
    "                    # Update summaries\n",
    "                    for q_key, content in data.items():\n",
    "                        if \"eval_databases\" in content:\n",
    "                            result = content[\"eval_databases\"]\n",
    "                            \n",
    "                            # Update total summary\n",
    "                            if result == \"fact\":\n",
    "                                total_summary[\"facts\"] += 1\n",
    "                            elif result == \"hallucination\":\n",
    "                                total_summary[\"hallucinations\"] += 1\n",
    "                            elif result == \"N/A\":\n",
    "                                total_summary[\"N/A\"] += 1\n",
    "                            \n",
    "                            total_summary[\"total_processed\"] += 1\n",
    "                            \n",
    "                            # Update combination-specific summary\n",
    "                            comb_key = f\"{granularity}_{input_type}\"\n",
    "                            combination_summaries[comb_key][q_key][result] += 1\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # Create the report\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"EVALUATION SUMMARY REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\\nOVERALL SUMMARY:\")\n",
    "    report.append(\"-\" * 40)\n",
    "    report.append(f\"Total questions processed: {total_summary['total_processed']}\")\n",
    "    report.append(f\"Facts: {total_summary['facts']}\")\n",
    "    report.append(f\"Hallucinations: {total_summary['hallucinations']}\")\n",
    "    report.append(f\"N/A: {total_summary['N/A']}\")\n",
    "    \n",
    "    # Add detailed summaries for each combination\n",
    "    for granularity in granularities:\n",
    "        for input_type in input_types:\n",
    "            comb_key = f\"{granularity}_{input_type}\"\n",
    "            report.append(f\"\\n\\nDETAILED SUMMARY FOR {comb_key.upper()}\")\n",
    "            report.append(\"-\" * 60)\n",
    "            \n",
    "            # Summary table header\n",
    "            report.append(\"\\nQuestion | Facts | Hallucinations | N/A\")\n",
    "            report.append(\"-\" * 50)\n",
    "            \n",
    "            # Add data for each question\n",
    "            for q_num in range(1, 16):\n",
    "                q_key = f\"Q{q_num}\"\n",
    "                q_data = combination_summaries[comb_key][q_key]\n",
    "                report.append(\n",
    "                    f\"{q_key:8} | {q_data['fact']:5} | {q_data['hallucination']:13} | {q_data['N/A']:11}\"\n",
    "                )\n",
    "    \n",
    "    # Save the report\n",
    "    report_path = \"evaluation_summary_report_1.txt\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    print(f\"\\nDetailed summary report has been saved to: {report_path}\")\n",
    "    \n",
    "    return total_summary, combination_summaries\n",
    "\n",
    "########## CODE EXECUTION ###################\n",
    "\n",
    "# Define possible values\n",
    "granularities = [\"Paragraph\"]         # [\"Paragraph\", \"Sentence\", \"Claim\"]\n",
    "input_types =   [\"title\", \"summary\", \"json\"]  # [\"title\", \"json\", \"summary\"]\n",
    "\n",
    "# Base path\n",
    "base_path = \"Database_dependent_evaluation/Clinical_trials/3-Inference/Model_Answer/Llama3.3-70B\"\n",
    "output_base_path = \"Database_dependent_evaluation/Clinical_trials/4-Evaluation/Evaluation_factual/Llama3.3-70B/Paragraph_level\"\n",
    "\n",
    "eval_vs_summary_base_path = \"Database_dependent_evaluation/Clinical_trials/4-Evaluation/Evaluation_factual/Llama3.3-70B/Paragraph_level\"\n",
    "\n",
    "# Loop through all combinations\n",
    "for granularity in granularities:\n",
    "    for input_type in input_types:\n",
    "        # Construct the full path\n",
    "        inference_input_dir = os.path.join(\n",
    "            base_path,\n",
    "            f\"{granularity}_level\",\n",
    "            f\"Inference_{input_type}_{granularity}\"\n",
    "        )\n",
    "\n",
    "        eval_vs_summary_input_dir = os.path.join(\n",
    "            eval_vs_summary_base_path,\n",
    "            f\"{granularity}_level\",\n",
    "            f\"Evaluation_{input_type}_{granularity}\"\n",
    "        )\n",
    "\n",
    "        output_dir = os.path.join(\n",
    "            output_base_path,\n",
    "            f\"{granularity}_level\",\n",
    "            f\"Inference_{input_type}_{granularity}\"\n",
    "        )\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Log or print the current combination being processed\n",
    "        print(f\"\\nProcessing: Granularity={granularity}, Input Type={input_type}\")\n",
    "        # print(f\"Input Inference Directory: {inference_input_dir}\")\n",
    "        # print(f\"Input Eval_vs_json Directory: {eval_vs_json_input_dir}\")\n",
    "        # print(f\"Input Eval_vs_summary Directory: {eval_vs_summary_input_dir}\")\n",
    "        \n",
    "            \n",
    "        # Get all JSON files in this directory\n",
    "        json_files = sorted([f for f in os.listdir(inference_input_dir) if f.endswith('.json')])\n",
    "        print(f\"Found {len(json_files)} JSON files in directory\")\n",
    "\n",
    "        # Process files\n",
    "        error_files = []\n",
    "        for json_file in tqdm(json_files, desc=f\"Processing {granularity}-{input_type}\"):\n",
    "            try:\n",
    "                inference_file_path= os.path.join(inference_input_dir, json_file)\n",
    "                eval_1_file_path = os.path.join(eval_vs_summary_input_dir, json_file)\n",
    "                output_file_path = os.path.join(output_dir, json_file)\n",
    "                # print(\"Inference:\",inference_file_path)\n",
    "                # print(\"Eval1:\",eval_1_file_path)\n",
    "                # print(\"Output:\",output_file_path)\n",
    "        \n",
    "                # Check if all files exist\n",
    "                for file_path in [inference_file_path, eval_1_file_path]:\n",
    "                    if not os.path.exists(file_path):\n",
    "                        error_msg = f\"File not found: {file_path}\"\n",
    "                        log_error(error_msg)\n",
    "                        error_files.append((json_file, error_msg))\n",
    "                        continue\n",
    "            \n",
    "                # # Get comparison results\n",
    "                results = get_scores(eval_1_file_path)\n",
    "        \n",
    "                if results is None:\n",
    "                    error_msg = f\"Error comparing scores for {json_file}\"\n",
    "                    log_error(error_msg)\n",
    "                    error_files.append((json_file, error_msg))\n",
    "                    continue\n",
    "            \n",
    "                # # Add evaluation results to inference_file placing the file in a new output_file Inference_with_labels\n",
    "                add_evaluation_to_file(results, inference_file_path, output_file_path)\n",
    "        \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error processing {json_file}: {str(e)}\"\n",
    "                log_error(error_msg)\n",
    "                error_files.append((json_file, error_msg))\n",
    "                continue\n",
    "        \n",
    "        # Print summary of errors\n",
    "        if error_files:\n",
    "            print(\"\\nFiles with errors:\")\n",
    "            for file_name, error in error_files:\n",
    "                print(f\"- {file_name}: {error}\")\n",
    "\n",
    "# Generate and print summary report\n",
    "total_summary, combination_summaries = create_summary_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70f7a0-dbaa-45e9-83be-2e3dddd7d6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d272c14-8628-47d9-8712-7695bcc22409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497e028-ebab-478e-9a8c-45ae9aae8d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344479d6-f131-4ba3-8820-b878b015af55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d474b2-1f52-46bd-a210-5f603775582a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc967647-52bf-4f7d-bc79-1634ef1f2a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7127f-f89b-424d-b146-331c38be67a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549fca4-08ec-483b-b30c-f8bf43d35d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a9e6b-2bd3-4bad-8daa-dff54fac5b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce037e-1e3b-4878-a693-38dcc7df28c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b8085-2e46-4a98-920d-63f27bbeb224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c867f17-7df8-46bd-8bd1-0d34d1651323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fd65e-c182-48b8-bbec-59d09154ba2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa-memo",
   "language": "python",
   "name": "fa-memo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
