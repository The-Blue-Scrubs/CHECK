# Clinical Trials Evaluation (Database-Dependent)

This directory contains the code and data for the database-dependent evaluation of large language models on clinical trial data. The evaluation pipeline is designed to assess the model's ability to answer questions based on a curated database of clinical trials, under both factual and counterfactual conditions.

## Experimental Workflow

The experiment is divided into several sequential steps, organized into numbered directories. To replicate the experiment, please follow the steps in order.

### 1. Curated Clinical Trial Database (`1-Curated_ClinicalTrial_database/`)

This directory is the destination for the curated clinical trial data, which must be downloaded from our Zenodo repository. Please refer to the main `README.md` in the root of the project for instructions on how to download the data.

### 2. Randomization and Filtering (`2-Randomize/`)

This step involves filtering the clinical trial database for specific criteria (e.g., cancer-related trials) and creating a randomized subset for the experiment.

- `filter_cancer_and_copy_files.ipynb`: Jupyter notebook to filter trials related to cancer.
- `Randomize.ipynb`: Jupyter notebook to randomize the filtered trials.

### 3. Inference (`3-Inference/`)

In this step, the language model is used to generate answers based on the context provided from the selected clinical trials.

- `Context/`: Contains the input context (questions and trial information) fed to the model.
- `Model_Answer/`: Contains the raw output (answers) generated by the model.

### 4. Evaluation (`4-Evaluation/`)

This step evaluates the model's generated answers against the ground truth labels. The evaluation is performed for both factual and counterfactual scenarios.

- `Evaluation_factual/`: Contains scripts and results for the factual evaluation.
- `Evaluation_counterfactual/`: Contains scripts and results for the counterfactual evaluation.

### 5. Labels (`5-Labels/`)

This directory contains the ground truth labels for the inference files. These labels are used during the evaluation step to assess the accuracy of the model's responses.

- `Inference_files_with_labels/`: Labels for the factual inference task.
- `Inference_files_with_labels_Counterfactual/`: Labels for the counterfactual inference task.

### 6. Analysis (`6-Analysis/`)

The final step involves analyzing the evaluation results to draw conclusions about the model's performance. The analysis is broken down by context and by model.

- `Data_by_context/`: Notebooks and data for analyzing performance based on the type of context provided.
- `Data_by_model/`: Notebooks and data for comparing the performance of different models.
