{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daed4ca4-1ea1-47ae-b706-35274a5b4196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "Total trials available: 18284\n",
      "Trials selected: 100\n",
      "Output saved to: trials_with_results.txt\n",
      "Random seed used: 42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_random_trials(input_file, output_file, num_trials=100, seed=42):\n",
    "    \"\"\"\n",
    "    Read trial paths from file, select random trials, and save their names\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: path to file containing trial paths\n",
    "    - output_file: path to save selected trial names\n",
    "    - num_trials: number of trials to select\n",
    "    - seed: random seed for reproducibility\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Read and process file paths\n",
    "        with open(input_file, 'r') as f:\n",
    "            all_trials = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        # Extract filenames and create a mapping\n",
    "        filename_map = {path.split('/')[-1]: path for path in all_trials}\n",
    "        filenames = list(filename_map.keys())\n",
    "        \n",
    "        # Validate number of trials\n",
    "        available_trials = len(filenames)\n",
    "        if num_trials > available_trials:\n",
    "            print(f\"Warning: Requested {num_trials} trials but only {available_trials} available\")\n",
    "            num_trials = available_trials\n",
    "        \n",
    "        # Select random trials\n",
    "        selected_trials = random.sample(filenames, num_trials)\n",
    "        \n",
    "        # Sort for readability\n",
    "        selected_trials.sort()\n",
    "        \n",
    "        # Save selected trial names\n",
    "        with open(output_file, 'w') as f:\n",
    "            for trial in selected_trials:\n",
    "                f.write(f\"{trial}\\n\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"Total trials available: {available_trials}\")\n",
    "        print(f\"Trials selected: {num_trials}\")\n",
    "        print(f\"Output saved to: {output_file}\")\n",
    "        print(f\"Random seed used: {seed}\")\n",
    "        \n",
    "        # Optional: return the full paths of selected trials\n",
    "        selected_paths = [filename_map[trial] for trial in selected_trials]\n",
    "        return selected_paths\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"cancer_trials.txt\"\n",
    "    output_file = \"trials_with_results.txt\"\n",
    "    \n",
    "    # Get random trials and their full paths\n",
    "    selected_paths = get_random_trials(\n",
    "        input_file=input_file,\n",
    "        output_file=output_file,\n",
    "        num_trials=100,\n",
    "        seed=42  # Set seed for reproducibility\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ffd4f9-76b6-4203-abad-a7ac60743a51",
   "metadata": {},
   "source": [
    "## Copy original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db5ceeeb-764e-42e2-bbaf-08d4eaaabcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "# Create destination folder\n",
    "destination_folder = \"Original_files_with_results\"\n",
    "source_folder = \"/home/4481281/Clinical_trials/Original_format/Original_format/\"\n",
    "trials_file = \"trials_with_results.txt\"\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "    \n",
    "# Read trial names\n",
    "with open(trials_file, 'r') as f:\n",
    "    trial_names = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "# Copy files\n",
    "copied = 0\n",
    "for trial_name in trial_names:\n",
    "    try:\n",
    "        # Assuming files are directly in numbered folders\n",
    "        for folder_num in range(1, 100):  # Adjust range as needed\n",
    "            source_path = os.path.join(source_folder, trial_name)\n",
    "            if os.path.exists(source_path):\n",
    "                shutil.copy2(source_path, os.path.join(destination_folder, trial_name))\n",
    "                copied += 1\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {trial_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e54bdc-6cc4-4240-97c8-edfba6943db3",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da886d0-3b73-4673-bfdb-04d349fed544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(data, question_number):\n",
    "    \"\"\"\n",
    "    Creates a prompt for a specific question number\n",
    "    \"\"\"\n",
    "    questions = {\n",
    "        1: \"Definition: Define the title and purpose of the clinical trial.\",\n",
    "        2: \"Condition: Describe the conditions studied.\",\n",
    "        3: \"Design Details: Explain how the study was designed, including the number of participants enrolled.\",\n",
    "        4: \"Interventions: Describe the interventions investigated in the clinical trial.\",\n",
    "        5: \"Study Arms: Explain how the study arms were structured.\",\n",
    "        6: \"Eligibility Criteria: Describe eligibility criteria for participation in the study.\",\n",
    "        7: \"Primary Outcome: Describe the primary outcome measured.\",\n",
    "        8: \"Primary Outcome Statistical Analysis: Describe the statistical methods used to analyze the primary outcome.\",\n",
    "        9: \"Primary Outcome Statistical Results: Summarize the statistical results obtained for the primary outcome.\",\n",
    "        10: \"Secondary Outcomes Overview: Provide a general summary of the secondary outcomes measured.\",\n",
    "        11: \"Statistical Approach: Briefly describe the statistical methods used to analyze the secondary outcomes.\",\n",
    "        12: (\"Key Results: Highlight the most important statistical results and clinically relevant findings from the secondary outcomes.\\n\"\n",
    "             \"- If no secondary outcomes are present, state: No secondary outcomes were measured in this clinical trial.\"),\n",
    "        13: \"Serious Adverse Events (SAEs): Summarize the most significant and clinically relevant serious adverse events reported.\",\n",
    "        14: (\"Non-Serious Adverse Events: Briefly list or group the most frequent non-serious adverse events highlighting those \"\n",
    "             \"that occurred most commonly or had the greatest impact on participant well-being.\"),\n",
    "        15: (\"Key Observations and Clinical Relevance: Provide a short overview of the overall safety profile based on the adverse events, \"\n",
    "             \"focusing on any notable trends or conclusions about tolerability and risk.\\n\"\n",
    "             \"If no adverse events are present, state: No adverse events were reported in this clinical trial.\")\n",
    "    }\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are an advanced clinical language model. Answer the following specific question about the clinical trial \"\n",
    "        f\"{data}. Follow the structure below to enhance clinical reasoning capabilities:\\n\\n\"\n",
    "        \n",
    "        f\"QUESTION:\\n\"\n",
    "        f\"{questions[question_number]}\\n\\n\"\n",
    "        \n",
    "        \"REQUIREMENTS:\\n\"\n",
    "        \"- Ensure clinical reasoning\\n\"\n",
    "        \"- Use medically precise terminology\\n\"\n",
    "        \"- Avoid introductory or concluding sentences\\n\"\n",
    "        \"- Be specific and focused on the requested aspect\\n\"\n",
    "        \"- If information is not available, explicitly state this\\n\\n\"\n",
    "        \n",
    "        \"RESPONSE:\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb84abe6-c270-47b8-9887-92f37bd2f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an advanced clinical language model. Answer the following specific question about the clinical trial trial 2. Follow the structure below to enhance clinical reasoning capabilities:\n",
      "\n",
      "QUESTION:\n",
      "Condition: Describe the conditions studied.\n",
      "\n",
      "REQUIREMENTS:\n",
      "- Ensure clinical reasoning\n",
      "- Use medically precise terminology\n",
      "- Avoid introductory or concluding sentences\n",
      "- Be specific and focused on the requested aspect\n",
      "- If information is not available, explicitly state this\n",
      "\n",
      "RESPONSE:\n"
     ]
    }
   ],
   "source": [
    "prompts = create_prompt(\"trial 2\", 2)\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719f278-671f-4cf1-8760-3625da0b3554",
   "metadata": {},
   "source": [
    "## Files for GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44408bff-5236-4060-92ef-31cbde4eca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 trials to copy\n",
      "\n",
      "Copy Summary:\n",
      "Total files to copy: 100\n",
      "Successfully copied: 100\n",
      "Not found: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_trial_files(trials_list_file, source_folder, destination_folder):\n",
    "    \"\"\"\n",
    "    Copy files listed in trials_list_file from source_folder to destination_folder\n",
    "    \n",
    "    Parameters:\n",
    "    - trials_list_file: txt file containing trial filenames\n",
    "    - source_folder: folder containing original files (with subfolders)\n",
    "    - destination_folder: where to copy the files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create destination folder if it doesn't exist\n",
    "        os.makedirs(destination_folder, exist_ok=True)\n",
    "        \n",
    "        # Read trial names from file\n",
    "        with open(trials_list_file, 'r') as f:\n",
    "            trial_names = [line.strip() for line in f if line.strip()]\n",
    "            \n",
    "        print(f\"Found {len(trial_names)} trials to copy\")\n",
    "        \n",
    "        # Track progress\n",
    "        copied = []\n",
    "        not_found = []\n",
    "        \n",
    "        # Search and copy each file\n",
    "        for trial_name in trial_names:\n",
    "            # Search recursively for the file\n",
    "            matches = list(Path(source_folder).rglob(trial_name))\n",
    "            \n",
    "            if matches:\n",
    "                # Take the first match if multiple exist\n",
    "                source_path = matches[0]\n",
    "                dest_path = os.path.join(destination_folder, trial_name)\n",
    "                \n",
    "                try:\n",
    "                    shutil.copy2(source_path, dest_path)\n",
    "                    copied.append(trial_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error copying {trial_name}: {e}\")\n",
    "                    not_found.append(trial_name)\n",
    "            else:\n",
    "                not_found.append(trial_name)\n",
    "                \n",
    "        # Print summary\n",
    "        print(\"\\nCopy Summary:\")\n",
    "        print(f\"Total files to copy: {len(trial_names)}\")\n",
    "        print(f\"Successfully copied: {len(copied)}\")\n",
    "        print(f\"Not found: {len(not_found)}\")\n",
    "        \n",
    "        if not_found:\n",
    "            print(\"\\nFiles not found:\")\n",
    "            for file in not_found:\n",
    "                print(f\"- {file}\")\n",
    "            \n",
    "            # Save list of not found files\n",
    "            with open('not_found_trials.txt', 'w') as f:\n",
    "                for file in not_found:\n",
    "                    f.write(f\"{file}\\n\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "trials_list = \"trials_with_results.txt\"\n",
    "source_dir = \"../Files_for_GPT/\"\n",
    "dest_dir = \"TRIALS\"\n",
    "    \n",
    "copy_trial_files(trials_list, source_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb21fe-4a80-498a-96e1-ed6e4abbaf24",
   "metadata": {},
   "source": [
    "## Copy files with summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f61888-60bb-4e32-961d-c15ce658ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 100 out of 100 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create Summary folder\n",
    "os.makedirs(\"Files_with_Summary\", exist_ok=True)\n",
    "\n",
    "# Read file paths from text file\n",
    "with open(\"summary_files.txt\", 'r') as f:\n",
    "    file_paths = eval(f.read())\n",
    "\n",
    "# Copy each file\n",
    "copied = 0\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        shutil.copy2(file_path, os.path.join(\"Files_with_Summary\", filename))\n",
    "        copied += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {file_path}: {e}\")\n",
    "\n",
    "print(f\"Copied {copied} out of {len(file_paths)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b075f9f-c71c-42f3-967b-36743cb6ac4d",
   "metadata": {},
   "source": [
    "## Split Q at sentences level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d2a3dd3-973a-4991-a026-7c99e9490be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 JSON files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 453.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "Total files processed: 100\n",
      "Output folder: Inference_summary_Sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    \"\"\"Clean individual sentence\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    # Remove leading/trailing whitespace\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def split_text_to_sentences(text):\n",
    "    \"\"\"Split text into sentences with cleaning\"\"\"\n",
    "    # Clean the text\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Clean each sentence\n",
    "    sentences = [clean_sentence(s) for s in sentences if clean_sentence(s)]\n",
    "    return sentences\n",
    "\n",
    "def process_json_files(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process JSON files by splitting text into sentences for each question\n",
    "    \"\"\"\n",
    "    # Download NLTK data if needed\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    # Create output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all JSON files\n",
    "    json_files = [f for f in os.listdir(input_folder) if f.endswith('.json')]\n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    \n",
    "    # Process files with progress bar\n",
    "    for file_name in tqdm(json_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            input_path = os.path.join(input_folder, file_name)\n",
    "            output_path = os.path.join(output_folder, file_name)\n",
    "            \n",
    "            # Read input JSON\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Process each question\n",
    "            processed_data = {}\n",
    "            for i in range(1, 8):\n",
    "                key = f'Q{i}'\n",
    "                if key in data:\n",
    "                    sentences = split_text_to_sentences(data[key])\n",
    "                    processed_data[key] = sentences\n",
    "                    \n",
    "            # Save processed data\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(processed_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {file_name}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(f\"Total files processed: {len(json_files)}\")\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "\n",
    "####################################################\n",
    "input_folder = \"Inference_summary\"\n",
    "output_folder = \"Inference_summary_Sentence\"\n",
    "    \n",
    "process_json_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a6988-afae-47fd-af52-16a698913ef0",
   "metadata": {},
   "source": [
    "## Split GPT test data (disorders) to sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81724689-8695-4008-bdd1-39a5208d3715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NCT047.json...\n",
      "Processing NCT109.json...\n",
      "Processing NCT039.json...\n",
      "Processing NCT055.json...\n",
      "Processing NCT064.json...\n",
      "Processing NCT083.json...\n",
      "Processing NCT108.json...\n",
      "Processing NCT035.json...\n",
      "Processing NCT010.json...\n",
      "Processing NCT024.json...\n",
      "Processing NCT119.json...\n",
      "Processing NCT058.json...\n",
      "Processing NCT077.json...\n",
      "Processing NCT071.json...\n",
      "Processing NCT001.json...\n",
      "Processing NCT091.json...\n",
      "Processing NCT018.json...\n",
      "Processing NCT115.json...\n",
      "Processing NCT004.json...\n",
      "Processing NCT015.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')  # Download the punkt tokenizer data\n",
    "\n",
    "def split_paragraphs_to_sentences(input_file: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Split Q1-Q7 paragraphs into sentences while maintaining file structure\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input JSON file\n",
    "        output_dir: Directory where the split JSON files will be saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Read input file\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Common fields that should not be split\n",
    "        common_fields = ['index', 'AUI', 'CUI', 'term', 'Definition', 'reasoning', 'statement']\n",
    "        \n",
    "        # Process the file\n",
    "        processed_data = {}\n",
    "        \n",
    "        # Copy common fields\n",
    "        for field in common_fields:\n",
    "            processed_data[field] = data.get(field, '')\n",
    "        \n",
    "        # Process Q1-Q7 fields\n",
    "        for i in range(1, 8):\n",
    "            q_key = f'Q{i}'\n",
    "            if q_key in data and data[q_key]:\n",
    "                # Split paragraph into sentences\n",
    "                sentences = sent_tokenize(data[q_key])\n",
    "                # Store as dictionary with sentence numbers\n",
    "                processed_data[q_key] = {\n",
    "                    f'{j}': sentence.strip()\n",
    "                    for j, sentence in enumerate(sentences)\n",
    "                }\n",
    "            else:\n",
    "                processed_data[q_key] = {}\n",
    "        \n",
    "        # Save processed data\n",
    "        output_file = os.path.join(output_dir, os.path.basename(input_file))\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return processed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {input_file}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_all_files(input_dir: str, output_dir: str):\n",
    "    \"\"\"Process all JSON files in the input directory\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Process each file\n",
    "        for filename in os.listdir(input_dir):\n",
    "            if filename.endswith('.json'):\n",
    "                input_path = os.path.join(input_dir, filename)\n",
    "                print(f\"Processing {filename}...\")\n",
    "                split_paragraphs_to_sentences(input_path, output_dir)\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing directory: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"/home/4481281/Clinical_trials/Original_format/Results/Factuality_Eval/Inference_files/UMLS/Paragraph_level\"\n",
    "    output_dir = \"/home/4481281/Clinical_trials/Original_format/Results/Factuality_Eval/Inference_files/UMLS/Sentence_level\"\n",
    "    process_all_files(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06b2da-697e-4607-a40b-ccae4460f42f",
   "metadata": {},
   "source": [
    "## Check Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3787f67-2260-4095-b03f-85010431d76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 64/100 [00:00<00:00, 319.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short item found in NCT00136916.json, Q11:\n",
      "Text: Not applicable.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT00136916.json, Q12:\n",
      "Text: Not applicable.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT00448136.json, Q1:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q1:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q1:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q1:\n",
      "Text: **Design Details\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT00448136.json, Q2:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q2:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q2:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q2:\n",
      "Text: 5.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q2:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q2:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q2:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q2:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q2:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q4:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q4:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q9:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q9:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q9:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q9:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q9:\n",
      "Text: 5.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q9:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q9:\n",
      "Text: 8.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q9:\n",
      "Text: 9.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q9:\n",
      "Text: 10.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q13:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q13:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q14:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00448136.json, Q14:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00580983.json, Q6:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00656513.json, Q4:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT00656513.json, Q8:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT00661544.json, Q12:\n",
      "Text: * This\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q1:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q2:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q3:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q4:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 5.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 8.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 9.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 10.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 11.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 12.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 13.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 14.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q5:\n",
      "Text: 15.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q7:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q8:\n",
      "Text: 8.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 5.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 8.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 9.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 10.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 11.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 12.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 13.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 14.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q9:\n",
      "Text: 15.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q10:\n",
      "Text: 10.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q11:\n",
      "Text: 11.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q12:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q12:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q12:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q12:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q12:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q12:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q13:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q14:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q15:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q15:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00667459.json, Q15:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT00726986.json, Q15:\n",
      "Text: The notable safety\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT00818961.json, Q3:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT00818961.json, Q5:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT00818961.json, Q8:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT00818961.json, Q14:\n",
      "Text: None reported.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT00883116.json, Q7:\n",
      "Text: Overall\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT00883116.json, Q12:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT00997334.json, Q4:\n",
      "Text: *\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01031381.json, Q2:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q3:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q4:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q5:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q6:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q7:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q8:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q8:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q8:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q8:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q9:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q10:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q10:\n",
      "Text: 10.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q10:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q11:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q12:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q12:\n",
      "Text: **Cause\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q13:\n",
      "Text: 13.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q14:\n",
      "Text: 13.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q15:\n",
      "Text: 13.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q15:\n",
      "Text: 14.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01031381.json, Q15:\n",
      "Text: 15.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01054911.json, Q12:\n",
      "Text: No serious adverse\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT01071915.json, Q4:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01071915.json, Q5:\n",
      "Text: The\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01218516.json, Q13:\n",
      "Text: The most\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01226485.json, Q2:\n",
      "Text: Advanced Cancer.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT01239355.json, Q9:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01320826.json, Q1:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01320826.json, Q1:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01320826.json, Q1:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01320826.json, Q1:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01320826.json, Q1:\n",
      "Text: 5.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01530997.json, Q1:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01530997.json, Q2:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT01530997.json, Q3:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01530997.json, Q6:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01530997.json, Q8:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01777945.json, Q4:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01777945.json, Q7:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01777945.json, Q8:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01838200.json, Q3:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01838200.json, Q7:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT01838200.json, Q12:\n",
      "Text: The\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: Thanks!\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02234115.json, Q2:\n",
      "Text: The final answer\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q8:\n",
      "Text: Thank you.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02234115.json, Q10:\n",
      "Text: }$\"\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02334423.json, Q3:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT02407990.json, Q1:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q2:\n",
      "Text: Advanced Cancer.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT02407990.json, Q3:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q3:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q5:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q6:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q7:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q8:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q9:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q10:\n",
      "Text: 10.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q11:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT02407990.json, Q12:\n",
      "Text: 10.\n",
      "Token count: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 307.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short item found in NCT03003949.json, Q1:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q1:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q2:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q3:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q3:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q3:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q3:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q4:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q6:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q6:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q6:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q6:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q6:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03003949.json, Q6:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03009058.json, Q3:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT03158220.json, Q3:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT03319173.json, Q11:\n",
      "Text: }$\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT03406858.json, Q15:\n",
      "Text: The adverse event\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT03530696.json, Q4:\n",
      "Text: The combination\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03530917.json, Q2:\n",
      "Text: Healthy Participants.\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT03775486.json, Q1:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q1:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q6:\n",
      "Text: ```\n",
      "Token count: 2\n",
      "\n",
      "Short item found in NCT03775486.json, Q7:\n",
      "Text: Measurement: Progress\n",
      "Token count: 4\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 5.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 8.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 9.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q9:\n",
      "Text: 10.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 5.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 6.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 7.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 8.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 9.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 10.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 11.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 12.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 13.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q12:\n",
      "Text: 14.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q15:\n",
      "Text: 1.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q15:\n",
      "Text: 2.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q15:\n",
      "Text: 3.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03775486.json, Q15:\n",
      "Text: 4.\n",
      "Token count: 3\n",
      "\n",
      "Short item found in NCT03875313.json, Q11:\n",
      "Text: Not applicable.\n",
      "Token count: 4\n",
      "\n",
      "Analysis Results:\n",
      "Total items analyzed: 4911\n",
      "Items with less than 5 tokens: 259\n",
      "Percentage: 5.27%\n",
      "\n",
      "Breakdown by question:\n",
      "Q1: 16/225 (7.11%)\n",
      "Q2: 33/216 (15.28%)\n",
      "Q3: 14/455 (3.08%)\n",
      "Q4: 10/306 (3.27%)\n",
      "Q5: 19/380 (5.00%)\n",
      "Q6: 27/250 (10.80%)\n",
      "Q7: 22/280 (7.86%)\n",
      "Q8: 20/336 (5.95%)\n",
      "Q9: 37/429 (8.62%)\n",
      "Q10: 6/266 (2.26%)\n",
      "Q11: 6/141 (4.26%)\n",
      "Q12: 27/619 (4.36%)\n",
      "Q13: 5/216 (2.31%)\n",
      "Q14: 5/202 (2.48%)\n",
      "Q15: 12/590 (2.03%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_token_counts(input_folder: str, model_name: str = \"meta-llama/Llama-2-70b-hf\"):\n",
    "    \"\"\"\n",
    "    Analyze token counts in JSON files and report items with less than 5 tokens.\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Path to folder containing JSON files\n",
    "        model_name: Name of the model for tokenizer\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/proj/lab_valdes/models/HF_models/Llama-3.3-70B-Instruct/\")\n",
    "    \n",
    "    # Initialize counters\n",
    "    total_items = 0\n",
    "    short_items = 0\n",
    "    short_items_by_q = {f\"Q{i}\": 0 for i in range(1, 16)}\n",
    "    total_items_by_q = {f\"Q{i}\": 0 for i in range(1, 16)}\n",
    "    \n",
    "    # Get list of JSON files\n",
    "    all_files = [f for f in os.listdir(input_folder) if f.endswith('.json')]\n",
    "    json_files = sorted(all_files)\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in tqdm(json_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # Process each question\n",
    "            for q_num in range(1, 16):\n",
    "                key = f\"Q{q_num}\"\n",
    "                if key in data:\n",
    "                    items = data[key]\n",
    "                    if not isinstance(items, list):\n",
    "                        items = [items]\n",
    "                    \n",
    "                    # Count tokens for each item\n",
    "                    for item in items:\n",
    "                        tokens = tokenizer.encode(item)\n",
    "                        token_count = len(tokens)\n",
    "                        \n",
    "                        total_items += 1\n",
    "                        total_items_by_q[key] += 1\n",
    "                        \n",
    "                        if token_count < 5:\n",
    "                            short_items += 1\n",
    "                            short_items_by_q[key] += 1\n",
    "                            print(f\"\\nShort item found in {filename}, {key}:\")\n",
    "                            print(f\"Text: {item}\")\n",
    "                            print(f\"Token count: {token_count}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    print(f\"Total items analyzed: {total_items}\")\n",
    "    print(f\"Items with less than 5 tokens: {short_items}\")\n",
    "    print(f\"Percentage: {(short_items/total_items)*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nBreakdown by question:\")\n",
    "    for q_num in range(1, 16):\n",
    "        key = f\"Q{q_num}\"\n",
    "        total = total_items_by_q[key]\n",
    "        short = short_items_by_q[key]\n",
    "        if total > 0:\n",
    "            percentage = (short/total)*100\n",
    "            print(f\"{key}: {short}/{total} ({percentage:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"Inference_summary_Sentence\"  # Change this to your input folder path\n",
    "    analyze_token_counts(input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45faff5-8fd8-4a3e-9b7c-d0b3df0c39f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
