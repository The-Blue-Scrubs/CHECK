# Database-Dependent Evaluation

This directory contains experiments for the database-dependent evaluation of large language models. These evaluations are characterized by their reliance on a specific, curated database to assess model performance, typically on question-answering tasks where the context is drawn directly from the database.

The following subdirectories contain the different database-dependent evaluation benchmarks:

- `Clinical_trials/`: Evaluation on a curated database of clinical trials. This experiment assesses the model's ability to answer questions based on provided trial information.

Please refer to the `README.md` file in the subdirectory for detailed instructions on how to replicate the experiment.
